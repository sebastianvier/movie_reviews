{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7cd9b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python in build modules:\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "## EDA libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "## Metrics (sklearn)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "\n",
    "## Models (sklearn)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ec5ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(y_true, prediction):\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix(y_true, prediction))\n",
    "    print(\"\\n\")\n",
    "    print('Classification Report')\n",
    "    print(classification_report(y_true, prediction))\n",
    "    print(\"\\n\")\n",
    "    print(\"Other Metrics:\")\n",
    "    print(f'Pression Score: {precision_score(y_true, prediction)}')\n",
    "    print(f'Accuracy Score: {accuracy_score(y_true, prediction)}')\n",
    "    print(f'Recall Score: {recall_score(y_true, prediction)}')\n",
    "    print(f'f1 Score {f1_score(y_true, prediction)}')\n",
    "    \n",
    "def check_model(clf, X_test, y_test):\n",
    "    prediction = clf.predict(X_test)\n",
    "    print_report(y_test, prediction)\n",
    "    \n",
    "def vectorize_X(vectorizer, X_train, X_test):\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized  = vectorizer.transform(X_test)\n",
    "    return X_train_vectorized, X_test_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95ef8b2",
   "metadata": {},
   "source": [
    "### 1) Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd00805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "## Importing the data\n",
    "\n",
    "path = 'data/train/'\n",
    "\n",
    "count = 0\n",
    "labels = []\n",
    "contents = []\n",
    "\n",
    "for label in ['neg','pos']:\n",
    "    filenames = os.listdir(path + label)\n",
    "    for filename in filenames:\n",
    "        count += 1\n",
    "        with open(os.path.join(path, label, filename), 'r') as f:\n",
    "            labels.append(1 if label == 'pos' else 0) # 1 is positve 0 is negative\n",
    "            contents.append(f.read())\n",
    "print(count)\n",
    "            \n",
    "data = pd.DataFrame({\n",
    "    'contents' : contents,\n",
    "    'labels': labels,\n",
    "\n",
    "})\n",
    "\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True) # This code will shuffle the data (just in case!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdecdb1c",
   "metadata": {},
   "source": [
    "#### Just to be clear negative is 0 and positive is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1935c7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12500\n",
       "1    12500\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab00242",
   "metadata": {},
   "source": [
    "Ok, the data is **completely balanced**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298663a9",
   "metadata": {},
   "source": [
    " ###### At this moment we are going to see 5 examples of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65b3dd8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "\n",
      "\n",
      "Seriously, I'm all for gooey romantic comedies and will get sucked into Miss Congeniality as easily as Goodfellas...but this movie? It doesn't make any sense!!!! And I'm not even talking about the willing suspension of disbelief kind of not making sense. Why does her family live in England? Or, at the very least, why doesn't she have a British accent? She's sure cozy with her dad and he's surprisingly forgiving of her not being around for the last two years. (On that subject, no one ever makes much of a deal about her being away for so long). And what was with the goofy outfits at the bachelorette party? I'm not even going to get into the fact that the escort she paid for falls in love with her--that could've been overcome by better movie-making. I'm just saying that the characters, the setting, and the plot aren't fleshed out enough to make an even somewhat cohesive story. Oh, and the worst part, in my opinion, is the filmmaker's consistent use of the most unflattering angles on Deborah Messing's nose--I'd have sued the filmmakers if I were her! I mean, honestly, I'm all for women being who they are, but why, in seven loyal years of Will and Grace viewing, have I not ever noticed how incredibly odd her nose is? Oh! Because those producers are kind to her! This movie, like my other least favorite movie ever, Armageddon, is the fault of the filmmakers, not the actors. I can see both Messing and McDermott in these roles with a better writer, director, and producer.<br /><br />This easily gets my vote as one of the worst movies I've ever wasted time on. I'm just glad a friend loaned me her DVD, so all I wasted was time. If there were a way to make this review ZERO stars, I'd do it.\n",
      "\n",
      "\n",
      "David Mamet is a very interesting and a very un-equal director. His first movie 'House of Games' was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.<br /><br />So is 'Homicide' which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.<br /><br />This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.<br /><br />Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.\n",
      "\n",
      "\n",
      "positive\n",
      "\n",
      "\n",
      "In watching this early DeMille work, it was once again reinforced to me that early DeMille is far superior to late DeMille. His attention to use of light within scenes is remarkable. His pacing is very good, enabling much to be told in the space of an hour or so. It is a pity that he wasn't as intuitive about the style of his later sound films as he seemed to be in his silent films.<br /><br />This was the first film in which I had seen Cleo Ridgely. She was remarkable, quite restrained and yet conveyed a broad spectrum of emotions.<br /><br />The ending is wonderful.\n",
      "\n",
      "\n",
      "I have passed several times on watching this since I figured it was some dumb, sappy, dated romantic comedy. Well, it is a romantic comedy, and maybe a little dated. However, it is not overly sentimental, touching as it does on themes of office politics, adultery, and loneliness. You think you know exactly where things are headed, but there is an element of unpredictability that keeps your interest, and not everything turns out quite as you had expected. But, there is enough wit and charm to touch the most inveterate cynic. If you meet someone who doesn't like this movie, seriously consider how well you want to know them.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for label in data.labels.unique():\n",
    "    print(\"negative\" if label ==  0 else \"positive\")\n",
    "    print(\"\\n\")\n",
    "    for content in data.contents[data.labels == label].sample(2):\n",
    "        print(content)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd2f5d0",
   "metadata": {},
   "source": [
    "###### From now on I am going to be using the training data to explore the data so I can get some conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99e48b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.contents\n",
    "y = data.labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54397f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_train, y_train],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edfbbc31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8387\n",
       "0    8363\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.labels.value_counts() # Just checking the proportions haven't change much"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9304e8",
   "metadata": {},
   "source": [
    "### Does it has to do something with length?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c034f50c",
   "metadata": {},
   "source": [
    "###### Mean of length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfb077e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of all reviews: 1328.28\n",
      "Mean length in positive reviews: 1352.3945391677596\n",
      "Mean length in negative reviews: 1304.1033122085375\n",
      "Ratio positive: 1.0181547107294844\n",
      "Ration negative: 0.9817985004731966\n"
     ]
    }
   ],
   "source": [
    "mean_length_full = np.round(X_train.map(len).mean(),2)\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "mean_lenght_pos = train_df[train_df.labels == 1].contents.map(len).mean()\n",
    "mean_length_neg = train_df[train_df.labels == 0].contents.map(len).mean()\n",
    "ratio_pos =  mean_lenght_pos / mean_length_full\n",
    "ratio_neg =  mean_length_neg / mean_length_full\n",
    "\n",
    "print(f'Mean length of all reviews: {mean_length_full}')\n",
    "print(f'Mean length in positive reviews: {mean_lenght_pos}')\n",
    "print(f'Mean length in negative reviews: {mean_length_neg}')\n",
    "print(f'Ratio positive: {ratio_pos}')\n",
    "print(f'Ration negative: {ratio_neg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd11a0",
   "metadata": {},
   "source": [
    "We can run an **A/B test** here, but there is an indication that the **length** of the commentary has **nothing to do** with the idea if it is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587a175",
   "metadata": {},
   "source": [
    "###### Describing of length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48c84568",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe the length of model when positive:\n",
      "count     8387.000000\n",
      "mean      1352.394539\n",
      "std       1053.673512\n",
      "min         70.000000\n",
      "25%        695.000000\n",
      "50%        982.000000\n",
      "75%       1659.500000\n",
      "max      13704.000000\n",
      "Name: contents, dtype: float64\n",
      "\n",
      "\n",
      "Describe the length of model when negative:\n",
      "count    8363.000000\n",
      "mean     1304.103312\n",
      "std       963.063593\n",
      "min        53.000000\n",
      "25%       710.000000\n",
      "50%       981.000000\n",
      "75%      1554.000000\n",
      "max      8754.000000\n",
      "Name: contents, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Describe the length of model when positive:')\n",
    "print(train_df[train_df.labels == 1].contents.map(len).describe())\n",
    "print('\\n')\n",
    "print('Describe the length of model when negative:')\n",
    "print(train_df[train_df.labels == 0].contents.map(len).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5e50f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage: 80\n",
      "positive\n",
      "1917.0\n",
      "negative\n",
      "1799.0\n",
      "percentage: 84\n",
      "positive\n",
      "2168.7199999999993\n",
      "negative\n",
      "2015.0\n",
      "percentage: 88\n",
      "positive\n",
      "2514.0\n",
      "negative\n",
      "2328.120000000001\n",
      "percentage: 92\n",
      "positive\n",
      "3005.0\n",
      "negative\n",
      "2755.12\n",
      "percentage: 96\n",
      "positive\n",
      "3815.119999999999\n",
      "negative\n",
      "3521.079999999998\n",
      "percentage: 100\n",
      "positive\n",
      "13704.0\n",
      "negative\n",
      "8754.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(80,101,4):\n",
    "    print(f'percentage: {i}')\n",
    "    print('positive')\n",
    "    print(np.percentile(train_df[train_df.labels == 1].contents.map(len), i))\n",
    "    print('negative')\n",
    "    print(np.percentile(train_df[train_df.labels == 0].contents.map(len), i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a21e679",
   "metadata": {},
   "source": [
    "###### Doesn't seem likely. We can see there is a little divergence when the comments start to get longer that people with good reviews will tend to write longer comments. #trustinhumanityrestored (not really)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe81516",
   "metadata": {},
   "source": [
    "### What about number of paragraphs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c06ac233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_paragraph(x):\n",
    "    return x.count('<br /><br />') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe17f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = pd.concat([train_df.contents.apply(count_paragraph), train_df.labels],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4db9804f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe the n parragraphs of model when positive:\n",
      "count    8387.000000\n",
      "mean        2.972696\n",
      "std         2.518918\n",
      "min         1.000000\n",
      "25%         1.000000\n",
      "50%         2.000000\n",
      "75%         4.000000\n",
      "max        26.000000\n",
      "Name: contents, dtype: float64\n",
      "\n",
      "\n",
      "Describe the n parragraphs of model when negative:\n",
      "count    8363.000000\n",
      "mean        3.116705\n",
      "std         2.797950\n",
      "min         1.000000\n",
      "25%         1.000000\n",
      "50%         2.000000\n",
      "75%         4.000000\n",
      "max        47.000000\n",
      "Name: contents, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Describe the n parragraphs of model when positive:')\n",
    "print(paragraphs[paragraphs.labels == 1].contents.describe())\n",
    "print('\\n')\n",
    "print('Describe the n parragraphs of model when negative:')\n",
    "print(paragraphs[paragraphs.labels == 0].contents.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6bb3e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage: 80\n",
      "positive\n",
      "5.0\n",
      "negative\n",
      "5.0\n",
      "percentage: 84\n",
      "positive\n",
      "5.0\n",
      "negative\n",
      "5.0\n",
      "percentage: 88\n",
      "positive\n",
      "6.0\n",
      "negative\n",
      "6.0\n",
      "percentage: 92\n",
      "positive\n",
      "7.0\n",
      "negative\n",
      "7.0\n",
      "percentage: 96\n",
      "positive\n",
      "8.0\n",
      "negative\n",
      "8.0\n",
      "percentage: 100\n",
      "positive\n",
      "26.0\n",
      "negative\n",
      "47.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(80,101,4):\n",
    "    print(f'percentage: {i}')\n",
    "    print('positive')\n",
    "    print(np.percentile(paragraphs[paragraphs.labels == 1].contents, i))\n",
    "    print('negative')\n",
    "    print(np.percentile(paragraphs[paragraphs.labels == 0].contents , i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a34ba21",
   "metadata": {},
   "source": [
    "###### Doesn't seem likely.  There are cases that are outliers but nothing really there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e8f4bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can actually look for the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a93bd5f",
   "metadata": {},
   "source": [
    "### What about complexity and readiability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f47e3",
   "metadata": {},
   "source": [
    "###### Flesch ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72805488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FleschReadabilityEase(text):\n",
    "    if len(text) > 0:\n",
    "        total_words = len(text.split())\n",
    "        total_sentences = len(text.split('.'))\n",
    "        total_sillables = sum(list(map(lambda x: 1 if x in [\"a\",\"i\",\"e\",\"o\",\"u\",\"y\",\"A\",\"E\",\"I\",\"O\",\"U\",\"y\"] else 0, text)))\n",
    "        return 206.835 - (1.015 *  total_words/ total_sentences) - 84.6 * (total_sillables / total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5210724",
   "metadata": {},
   "outputs": [],
   "source": [
    "flesch_ease = train_df.contents.map(FleschReadabilityEase)\n",
    "flesch_ease = pd.concat([train_df, flesch_ease], axis=1)\n",
    "flesch_ease.columns = ['contents', 'labels', 'ease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36b02f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "      <th>labels</th>\n",
       "      <th>ease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12131</th>\n",
       "      <td>Ernst Lubitsch's contribution to the American ...</td>\n",
       "      <td>1</td>\n",
       "      <td>40.428436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12827</th>\n",
       "      <td>The name (Frau) of the main character is the G...</td>\n",
       "      <td>0</td>\n",
       "      <td>40.173923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>Upon The Straight Story release in 1999, it wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>34.813930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13762</th>\n",
       "      <td>This documentary explores a story covered in P...</td>\n",
       "      <td>1</td>\n",
       "      <td>23.340646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369</th>\n",
       "      <td>Tho 35 years old, Groove Tube looks a lot like...</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.854812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21575</th>\n",
       "      <td>Yes, Kazaam is one of those horribly bad movie...</td>\n",
       "      <td>0</td>\n",
       "      <td>35.520517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>The 3rd and in my view the best of the Blackad...</td>\n",
       "      <td>1</td>\n",
       "      <td>39.070429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>!!!!! POSSIBLE SPOILER !!!!!&lt;br /&gt;&lt;br /&gt;You`d ...</td>\n",
       "      <td>0</td>\n",
       "      <td>29.169732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>I cant understand at all why so many Godzilla ...</td>\n",
       "      <td>0</td>\n",
       "      <td>25.185830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23654</th>\n",
       "      <td>This movie is just plain silly. Almost every s...</td>\n",
       "      <td>1</td>\n",
       "      <td>61.992981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16750 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                contents  labels       ease\n",
       "12131  Ernst Lubitsch's contribution to the American ...       1  40.428436\n",
       "12827  The name (Frau) of the main character is the G...       0  40.173923\n",
       "2912   Upon The Straight Story release in 1999, it wa...       1  34.813930\n",
       "13762  This documentary explores a story covered in P...       1  23.340646\n",
       "6369   Tho 35 years old, Groove Tube looks a lot like...       1  -3.854812\n",
       "...                                                  ...     ...        ...\n",
       "21575  Yes, Kazaam is one of those horribly bad movie...       0  35.520517\n",
       "5390   The 3rd and in my view the best of the Blackad...       1  39.070429\n",
       "860    !!!!! POSSIBLE SPOILER !!!!!<br /><br />You`d ...       0  29.169732\n",
       "15795  I cant understand at all why so many Godzilla ...       0  25.185830\n",
       "23654  This movie is just plain silly. Almost every s...       1  61.992981\n",
       "\n",
       "[16750 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flesch_ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb1b1324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe the ease of model when positive:\n",
      "count    8387.000000\n",
      "mean       38.949473\n",
      "std        14.656158\n",
      "min      -134.725000\n",
      "25%        30.794855\n",
      "50%        39.755331\n",
      "75%        48.474976\n",
      "max        85.408412\n",
      "Name: ease, dtype: float64\n",
      "\n",
      "\n",
      "Describe the ease of model when negative:\n",
      "count    8363.000000\n",
      "mean       41.710220\n",
      "std        16.602777\n",
      "min      -337.909000\n",
      "25%        34.047744\n",
      "50%        42.679525\n",
      "75%        51.194469\n",
      "max       103.367105\n",
      "Name: ease, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Describe the ease of model when positive:')\n",
    "print(flesch_ease[flesch_ease.labels == 1].ease.describe())\n",
    "print('\\n')\n",
    "print('Describe the ease of model when negative:')\n",
    "print(flesch_ease[flesch_ease.labels == 0].ease.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af616585",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage: 80\n",
      "positive\n",
      "50.54745520133298\n",
      "negative\n",
      "53.188188158577596\n",
      "percentage: 84\n",
      "positive\n",
      "52.426362848962086\n",
      "negative\n",
      "55.0011225702047\n",
      "percentage: 88\n",
      "positive\n",
      "54.443156091257045\n",
      "negative\n",
      "56.96770877192985\n",
      "percentage: 92\n",
      "positive\n",
      "57.18548441708349\n",
      "negative\n",
      "59.651001063020445\n",
      "percentage: 96\n",
      "positive\n",
      "61.48505739004381\n",
      "negative\n",
      "63.615432606322905\n",
      "percentage: 100\n",
      "positive\n",
      "85.4084121621622\n",
      "negative\n",
      "103.36710526315791\n"
     ]
    }
   ],
   "source": [
    "for i in range(80,101,4):\n",
    "    print(f'percentage: {i}')\n",
    "    print('positive')\n",
    "    print(np.percentile(flesch_ease[flesch_ease.labels == 1].ease, i))\n",
    "    print('negative')\n",
    "    print(np.percentile(flesch_ease[flesch_ease.labels == 0].ease, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a11019b",
   "metadata": {},
   "source": [
    "###### The difference in complexity is not crazy, but is there (at least for this model) . Can we use this knowledge in some sort of way? The process is also a little bit memory consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cab672",
   "metadata": {},
   "source": [
    "##### There are outliers also outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "697e4e70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This, the finest achievement from Georg Wilhelm Pabst\\'s Social Realism period is based upon a tragedy in early 1906 that claimed the lives of nearly 1100 French miners as a coal dust explosion deep in mines at Courrieres in northern France took place after a fire had smouldered for three weeks, eventually releasing deadly pit gas that brought about the fatalities. Estimable designer Erno Metzner creates stark sets that simulate the tragedy, providing a perception of reality, augmented by matchless sound editing, with the only music being produced by integral orchestras during the beginning and ending portions of a work for which aural effects possess equal importance with the eminent director\\'s fascinating visual compositions. Pabst\\'s manner of \"invisible editing\" that segues action from shot to shot through movements of players proves to be smoothly integrated within this landmark film that also showcases sublime cinematography utilizing cameras mounted upon vehicles, enabling the director to shift amid scenes without having a necessity of cutting. Although the work\\'s cardinal theme relates to Socialist dogma, the unforgettable power of this film is held in its details, born of Pabst\\'s nonpareil skill at weaving numerous plot lines into a cinema tapestry that stirs one to admiration for German rescue squads of whom their Fatherland is greatly proud while no less despairing of disastrous losses to the families of French victims; certainly, a seminal triumph fully as stimulating today to a cineaste as it was at the time of its first release.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Outliers of low understandability\n",
    "flesch_ease[flesch_ease.ease < -0].contents.sample(1).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a58c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are reviews that make little sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ef8cc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flesch_ease[flesch_ease.ease < -0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f905b021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i just saw this movie on TV..<br /><br />i've lost my dad when i was young and this movie surely did touch me..<br /><br />i can feel the lost that the little girl Desi felt..<br /><br />the feeling of wanting to see her father again..<br /><br />wanting to talk to him..<br /><br />or at least given the chance to say goodbye..<br /><br />and i'm so touched with the letter that was wrote back to her..<br /><br />saying that her father read her letter, and sent it back to someone to reply her and buy her a present because there isn't a shop in heaven..<br /><br />it just lets me feel that miracles do exist..\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Outliers of high understandability\n",
    "flesch_ease[flesch_ease.ease > 80].contents.sample(1).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb7428fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flesch_ease[flesch_ease.ease > 80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2be9359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "      <th>labels</th>\n",
       "      <th>ease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12131</th>\n",
       "      <td>Ernst Lubitsch's contribution to the American ...</td>\n",
       "      <td>1</td>\n",
       "      <td>40.428436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12827</th>\n",
       "      <td>The name (Frau) of the main character is the G...</td>\n",
       "      <td>0</td>\n",
       "      <td>40.173923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>Upon The Straight Story release in 1999, it wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>34.813930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13762</th>\n",
       "      <td>This documentary explores a story covered in P...</td>\n",
       "      <td>1</td>\n",
       "      <td>23.340646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369</th>\n",
       "      <td>Tho 35 years old, Groove Tube looks a lot like...</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.854812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21575</th>\n",
       "      <td>Yes, Kazaam is one of those horribly bad movie...</td>\n",
       "      <td>0</td>\n",
       "      <td>35.520517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>The 3rd and in my view the best of the Blackad...</td>\n",
       "      <td>1</td>\n",
       "      <td>39.070429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>!!!!! POSSIBLE SPOILER !!!!!&lt;br /&gt;&lt;br /&gt;You`d ...</td>\n",
       "      <td>0</td>\n",
       "      <td>29.169732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>I cant understand at all why so many Godzilla ...</td>\n",
       "      <td>0</td>\n",
       "      <td>25.185830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23654</th>\n",
       "      <td>This movie is just plain silly. Almost every s...</td>\n",
       "      <td>1</td>\n",
       "      <td>61.992981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16750 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                contents  labels       ease\n",
       "12131  Ernst Lubitsch's contribution to the American ...       1  40.428436\n",
       "12827  The name (Frau) of the main character is the G...       0  40.173923\n",
       "2912   Upon The Straight Story release in 1999, it wa...       1  34.813930\n",
       "13762  This documentary explores a story covered in P...       1  23.340646\n",
       "6369   Tho 35 years old, Groove Tube looks a lot like...       1  -3.854812\n",
       "...                                                  ...     ...        ...\n",
       "21575  Yes, Kazaam is one of those horribly bad movie...       0  35.520517\n",
       "5390   The 3rd and in my view the best of the Blackad...       1  39.070429\n",
       "860    !!!!! POSSIBLE SPOILER !!!!!<br /><br />You`d ...       0  29.169732\n",
       "15795  I cant understand at all why so many Godzilla ...       0  25.185830\n",
       "23654  This movie is just plain silly. Almost every s...       1  61.992981\n",
       "\n",
       "[16750 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flesch_ease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a5d12",
   "metadata": {},
   "source": [
    "###### So how is the data distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e0cc037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.boxplot(x=\"labels\", y=\"ease\",\n",
    "                    data=flesch_ease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ad539cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.boxplot(x=\"labels\", y=\"ease\",\n",
    "                    data=flesch_ease[flesch_ease.ease > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71263b00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.boxplot(x=\"labels\", y=\"ease\",\n",
    "                    data=flesch_ease[flesch_ease.ease > 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35979931",
   "metadata": {},
   "source": [
    "###### There is a difference but not that much, when you take outliers positive reviews are just a bit harder to read. At least in this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58751a6",
   "metadata": {},
   "source": [
    "###### Most common word for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d57b119",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>226374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>110239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>97692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>91327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>72318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>br</td>\n",
       "      <td>68495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>it</td>\n",
       "      <td>65011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td>62845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>this</td>\n",
       "      <td>50974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>that</td>\n",
       "      <td>49116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>was</td>\n",
       "      <td>32075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>as</td>\n",
       "      <td>31526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>for</td>\n",
       "      <td>29811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>movie</td>\n",
       "      <td>29732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>with</td>\n",
       "      <td>29532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>but</td>\n",
       "      <td>28659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>film</td>\n",
       "      <td>26875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>you</td>\n",
       "      <td>23086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>on</td>\n",
       "      <td>22838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>not</td>\n",
       "      <td>20462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words  counts\n",
       "0     the  226374\n",
       "1     and  110239\n",
       "2      of   97692\n",
       "3      to   91327\n",
       "4      is   72318\n",
       "5      br   68495\n",
       "6      it   65011\n",
       "7      in   62845\n",
       "8    this   50974\n",
       "9    that   49116\n",
       "10    was   32075\n",
       "11     as   31526\n",
       "12    for   29811\n",
       "13  movie   29732\n",
       "14   with   29532\n",
       "15    but   28659\n",
       "16   film   26875\n",
       "17    you   23086\n",
       "18     on   22838\n",
       "19    not   20462"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Most common words in general\n",
    "\n",
    "cv = CountVectorizer(stop_words={'english'})\n",
    "list_of_words = cv.fit_transform(X_train).toarray()\n",
    "sum_of_words = pd.Series(list_of_words.sum(axis=0))\n",
    "sum_of_words = sum_of_words.sort_values(ascending=False)[:20]\n",
    "\n",
    "words = []\n",
    "numbers = []\n",
    "\n",
    "for num in sum_of_words.index:\n",
    "    word = (list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(num)])\n",
    "    numbers.append(sum_of_words.loc[num])\n",
    "    words.append(word)\n",
    "    \n",
    "pd.DataFrame({\n",
    "    \"words\" : words,\n",
    "    \"counts\" : numbers,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e007398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>116902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>60296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>51458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>45150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>38653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in</td>\n",
       "      <td>33731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>br</td>\n",
       "      <td>33091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>it</td>\n",
       "      <td>32486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>that</td>\n",
       "      <td>23998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this</td>\n",
       "      <td>23519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as</td>\n",
       "      <td>17752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>with</td>\n",
       "      <td>15599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>for</td>\n",
       "      <td>15132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>was</td>\n",
       "      <td>14698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>film</td>\n",
       "      <td>14146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>but</td>\n",
       "      <td>14098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>movie</td>\n",
       "      <td>12845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>his</td>\n",
       "      <td>11603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>on</td>\n",
       "      <td>11422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>you</td>\n",
       "      <td>11294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words  counts\n",
       "0     the  116902\n",
       "1     and   60296\n",
       "2      of   51458\n",
       "3      to   45150\n",
       "4      is   38653\n",
       "5      in   33731\n",
       "6      br   33091\n",
       "7      it   32486\n",
       "8    that   23998\n",
       "9    this   23519\n",
       "10     as   17752\n",
       "11   with   15599\n",
       "12    for   15132\n",
       "13    was   14698\n",
       "14   film   14146\n",
       "15    but   14098\n",
       "16  movie   12845\n",
       "17    his   11603\n",
       "18     on   11422\n",
       "19    you   11294"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_X_train = train_df.contents[train_df.labels == 1]\n",
    "\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,1))\n",
    "list_of_words = cv.fit_transform(pos_X_train).toarray()\n",
    "sum_of_words = pd.Series(list_of_words.sum(axis=0))\n",
    "sum_of_words = sum_of_words.sort_values(ascending=False)[:20]\n",
    "\n",
    "words = []\n",
    "numbers = []\n",
    "\n",
    "for num in sum_of_words.index:\n",
    "    word = (list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(num)])\n",
    "    numbers.append(sum_of_words.loc[num])\n",
    "    words.append(word)\n",
    "    \n",
    "pd.DataFrame({\n",
    "    \"words\" : words,\n",
    "    \"counts\" : numbers,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0736ff54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>116902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>60296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>51458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>45150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>38653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in</td>\n",
       "      <td>33731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>br</td>\n",
       "      <td>33091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>it</td>\n",
       "      <td>32486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>that</td>\n",
       "      <td>23998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this</td>\n",
       "      <td>23519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as</td>\n",
       "      <td>17752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>with</td>\n",
       "      <td>15599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>for</td>\n",
       "      <td>15132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>was</td>\n",
       "      <td>14698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>film</td>\n",
       "      <td>14146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>but</td>\n",
       "      <td>14098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>movie</td>\n",
       "      <td>12845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>his</td>\n",
       "      <td>11603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>on</td>\n",
       "      <td>11422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>you</td>\n",
       "      <td>11294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words  counts\n",
       "0     the  116902\n",
       "1     and   60296\n",
       "2      of   51458\n",
       "3      to   45150\n",
       "4      is   38653\n",
       "5      in   33731\n",
       "6      br   33091\n",
       "7      it   32486\n",
       "8    that   23998\n",
       "9    this   23519\n",
       "10     as   17752\n",
       "11   with   15599\n",
       "12    for   15132\n",
       "13    was   14698\n",
       "14   film   14146\n",
       "15    but   14098\n",
       "16  movie   12845\n",
       "17    his   11603\n",
       "18     on   11422\n",
       "19    you   11294"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_X_train = train_df.contents[train_df.labels == 0]\n",
    "\n",
    "cv = CountVectorizer(stop_words={'english'})\n",
    "list_of_words = cv.fit_transform(pos_X_train).toarray()\n",
    "sum_of_words = pd.Series(list_of_words.sum(axis=0))\n",
    "sum_of_words = sum_of_words.sort_values(ascending=False)[:20]\n",
    "\n",
    "words = []\n",
    "numbers = []\n",
    "\n",
    "for num in sum_of_words.index:\n",
    "    word = (list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(num)])\n",
    "    numbers.append(sum_of_words.loc[num])\n",
    "    words.append(word)\n",
    "    \n",
    "pd.DataFrame({\n",
    "    \"words\" : words,\n",
    "    \"counts\" : numbers,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75815e",
   "metadata": {},
   "source": [
    "#### 2) Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a3e56711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>films</td>\n",
       "      <td>2526</td>\n",
       "      <td>films</td>\n",
       "      <td>2526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>character</td>\n",
       "      <td>2386</td>\n",
       "      <td>character</td>\n",
       "      <td>2386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movies</td>\n",
       "      <td>2382</td>\n",
       "      <td>movies</td>\n",
       "      <td>2382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>characters</td>\n",
       "      <td>2341</td>\n",
       "      <td>characters</td>\n",
       "      <td>2341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>man</td>\n",
       "      <td>2330</td>\n",
       "      <td>man</td>\n",
       "      <td>2330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>then</td>\n",
       "      <td>2308</td>\n",
       "      <td>then</td>\n",
       "      <td>2308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>show</td>\n",
       "      <td>2308</td>\n",
       "      <td>show</td>\n",
       "      <td>2308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>little</td>\n",
       "      <td>2182</td>\n",
       "      <td>little</td>\n",
       "      <td>2182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>where</td>\n",
       "      <td>2174</td>\n",
       "      <td>where</td>\n",
       "      <td>2174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>could</td>\n",
       "      <td>2152</td>\n",
       "      <td>could</td>\n",
       "      <td>2152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>being</td>\n",
       "      <td>2150</td>\n",
       "      <td>being</td>\n",
       "      <td>2150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>does</td>\n",
       "      <td>2106</td>\n",
       "      <td>does</td>\n",
       "      <td>2106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>any</td>\n",
       "      <td>2026</td>\n",
       "      <td>any</td>\n",
       "      <td>2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>over</td>\n",
       "      <td>2012</td>\n",
       "      <td>over</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>while</td>\n",
       "      <td>1944</td>\n",
       "      <td>while</td>\n",
       "      <td>1944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>know</td>\n",
       "      <td>1917</td>\n",
       "      <td>know</td>\n",
       "      <td>1917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>here</td>\n",
       "      <td>1869</td>\n",
       "      <td>here</td>\n",
       "      <td>1869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>did</td>\n",
       "      <td>1850</td>\n",
       "      <td>did</td>\n",
       "      <td>1850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>these</td>\n",
       "      <td>1847</td>\n",
       "      <td>these</td>\n",
       "      <td>1847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>years</td>\n",
       "      <td>1830</td>\n",
       "      <td>years</td>\n",
       "      <td>1830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words  counts       words  counts\n",
       "0        films    2526       films    2526\n",
       "1    character    2386   character    2386\n",
       "2       movies    2382      movies    2382\n",
       "3   characters    2341  characters    2341\n",
       "4          man    2330         man    2330\n",
       "5         then    2308        then    2308\n",
       "6         show    2308        show    2308\n",
       "7       little    2182      little    2182\n",
       "8        where    2174       where    2174\n",
       "9        could    2152       could    2152\n",
       "10       being    2150       being    2150\n",
       "11        does    2106        does    2106\n",
       "12         any    2026         any    2026\n",
       "13        over    2012        over    2012\n",
       "14       while    1944       while    1944\n",
       "15        know    1917        know    1917\n",
       "16        here    1869        here    1869\n",
       "17         did    1850         did    1850\n",
       "18       these    1847       these    1847\n",
       "19       years    1830       years    1830"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_X_train = train_df.contents[train_df.labels == 0]\n",
    "\n",
    "cv = CountVectorizer(stop_words={'english'}, max_df=.2)\n",
    "list_of_words = cv.fit_transform(pos_X_train).toarray()\n",
    "sum_of_words = pd.Series(list_of_words.sum(axis=0))\n",
    "sum_of_words = sum_of_words.sort_values(ascending=False)[:20]\n",
    "\n",
    "words = []\n",
    "numbers = []\n",
    "\n",
    "for num in sum_of_words.index:\n",
    "    word = (list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(num)])\n",
    "    numbers.append(sum_of_words.loc[num])\n",
    "    words.append(word)\n",
    "    \n",
    "df1 = pd.DataFrame({\n",
    "    \"words\" : words,\n",
    "    \"counts\" : numbers,\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "pos_X_train = train_df.contents[train_df.labels == 1]\n",
    "\n",
    "cv = CountVectorizer(stop_words={'english'}, max_df=.2)\n",
    "list_of_words = cv.fit_transform(pos_X_train).toarray()\n",
    "sum_of_words = pd.Series(list_of_words.sum(axis=0))\n",
    "sum_of_words = sum_of_words.sort_values(ascending=False)[:20]\n",
    "\n",
    "words = []\n",
    "numbers = []\n",
    "\n",
    "for num in sum_of_words.index:\n",
    "    word = (list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(num)])\n",
    "    numbers.append(sum_of_words.loc[num])\n",
    "    words.append(word)\n",
    "    \n",
    "df2 = pd.DataFrame({\n",
    "    \"words\" : words,\n",
    "    \"counts\" : numbers,\n",
    "})\n",
    "\n",
    "pd.concat([df1,df2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d71c93",
   "metadata": {},
   "source": [
    "### 2) Running the first models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a880f",
   "metadata": {},
   "source": [
    "#### Model number 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7d50c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07155c94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 3,718,004 \n",
      "The amount of time it took to vectorize was: 17.154719999999998\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3751  386]\n",
      " [ 530 3583]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      4137\n",
      "           1       0.90      0.87      0.89      4113\n",
      "\n",
      "    accuracy                           0.89      8250\n",
      "   macro avg       0.89      0.89      0.89      8250\n",
      "weighted avg       0.89      0.89      0.89      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.9027462836986646\n",
      "Accuracy Score: 0.888969696969697\n",
      "Recall Score: 0.8711402868952103\n",
      "f1 Score 0.886661717396684\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "for cv ,clf, clf_name in [(vec , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(vec, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a1d26",
   "metadata": {},
   "source": [
    "This is the base Multinominal only using basic stop words in english. It is using a basic ngram_range of 1,1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc6e38d",
   "metadata": {},
   "source": [
    "#### Model number 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f4e570d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 2,991,762 \n",
      "The amount of time it took to vectorize was: 13.267948000000004\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3663  474]\n",
      " [ 545 3568]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88      4137\n",
      "           1       0.88      0.87      0.88      4113\n",
      "\n",
      "    accuracy                           0.88      8250\n",
      "   macro avg       0.88      0.88      0.88      8250\n",
      "weighted avg       0.88      0.88      0.88      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.8827313211281543\n",
      "Accuracy Score: 0.8764848484848485\n",
      "Recall Score: 0.8674933138828106\n",
      "f1 Score 0.8750459840588596\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "\n",
    "for vec ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    \n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(vec, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eba6da",
   "metadata": {},
   "source": [
    "The second model performs better, specially in recall. However it takes too much time to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1188f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model number 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d499e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 2,441,663 \n",
      "The amount of time it took to vectorize was: 10.759179000000017\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3653  484]\n",
      " [ 550 3563]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88      4137\n",
      "           1       0.88      0.87      0.87      4113\n",
      "\n",
      "    accuracy                           0.87      8250\n",
      "   macro avg       0.87      0.87      0.87      8250\n",
      "weighted avg       0.87      0.87      0.87      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.8804052384482333\n",
      "Accuracy Score: 0.8746666666666667\n",
      "Recall Score: 0.8662776562120107\n",
      "f1 Score 0.8732843137254902\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1, 3))\n",
    "X_train_sample = X_train.sample(frac=.8, random_state=50)\n",
    "y_train_sample = y_train.sample(frac=.8, random_state=50)\n",
    "\n",
    "for vec ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    \n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(vec, X_train_sample, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train_sample)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b6fa3",
   "metadata": {},
   "source": [
    "Feeding the model with less amount of data; seems to make the model run worse, which was expected. I need to check with an a/b testing though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc5a1c",
   "metadata": {},
   "source": [
    "#### Model number 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "635a74ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 5,342,619 \n",
      "The amount of time it took to vectorize was: 21.06203400000001\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3643  494]\n",
      " [ 527 3586]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88      4137\n",
      "           1       0.88      0.87      0.88      4113\n",
      "\n",
      "    accuracy                           0.88      8250\n",
      "   macro avg       0.88      0.88      0.88      8250\n",
      "weighted avg       0.88      0.88      0.88      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.878921568627451\n",
      "Accuracy Score: 0.8762424242424243\n",
      "Recall Score: 0.8718696814976903\n",
      "f1 Score 0.8753814231661174\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,5))\n",
    "\n",
    "for vec ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    \n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(vec, X_train_sample, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train_sample)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ecd56",
   "metadata": {},
   "source": [
    "#### Model number 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e2ef4d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 3,718,004 \n",
      "The amount of time it took to vectorize was: 16.878854000000004\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3751  386]\n",
      " [ 530 3583]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      4137\n",
      "           1       0.90      0.87      0.89      4113\n",
      "\n",
      "    accuracy                           0.89      8250\n",
      "   macro avg       0.89      0.89      0.89      8250\n",
      "weighted avg       0.89      0.89      0.89      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.9027462836986646\n",
      "Accuracy Score: 0.888969696969697\n",
      "Recall Score: 0.8711402868952103\n",
      "f1 Score 0.886661717396684\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "stop_words = 'the and of to is in br it that this as with for was film movie his on are have be one'.split(' ')\n",
    "cv = CountVectorizer(stop_words=stop_words, ngram_range=(1,3))\n",
    "\n",
    "for cv ,clf, clf_name in [(vec , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(vec, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e4097",
   "metadata": {},
   "source": [
    "This was a mechanical way to extract words but I have an idea of how to authomate the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe20d32",
   "metadata": {},
   "source": [
    "### 3) Tweeking Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2071e",
   "metadata": {},
   "source": [
    "#### Max df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40bc6d6",
   "metadata": {},
   "source": [
    "Max_df and min_df are use to eliminate words based on some threshold. The idea is that some tokens instead of helping\n",
    "the model are only noise.\n",
    "<br>There are two basic approaches to handle this types of words. We can eliminate very common words(using **max_df**) or we can eliminate very rare words using min_df.\n",
    "<br><br> The idea with max_df is that if a word appears at least in a certain percentage of the sample then it should be removed.\n",
    "<br><br> The idea with min_df is that if a word appears in less document that in the theshold it should be ignored. \n",
    "<br><br> Notice that max_df will have a lot of overlapping with stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4783ca4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the shape for 0.4\n",
      "(16750, 3713413)\n",
      "This is the shape for 0.5\n",
      "(16750, 3713428)\n",
      "This is the shape for 0.6\n",
      "(16750, 3713440)\n",
      "This is the shape for 0.7\n",
      "(16750, 3713446)\n",
      "This is the shape for 0.8\n",
      "(16750, 3713448)\n",
      "This is the shape for 0.9\n",
      "(16750, 3713452)\n",
      "This is the shape for 1.0\n",
      "(16750, 3713457)\n"
     ]
    }
   ],
   "source": [
    "for i in range(4,11,1):\n",
    "    count_vectorizer = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), max_df=i/10)\n",
    "    X_train_vectorized = count_vectorizer.fit_transform(X_train)\n",
    "    print(f'This is the shape for {i/10}')\n",
    "    print(X_train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9808131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the shape for 0.4\n",
      "(16750, 63655)\n",
      "This is the shape for 0.5\n",
      "(16750, 63670)\n",
      "This is the shape for 0.6\n",
      "(16750, 63680)\n",
      "This is the shape for 0.7\n",
      "(16750, 63685)\n",
      "This is the shape for 0.8\n",
      "(16750, 63687)\n",
      "This is the shape for 0.9\n",
      "(16750, 63691)\n",
      "This is the shape for 1.0\n",
      "(16750, 63696)\n"
     ]
    }
   ],
   "source": [
    "for i in range(4,11,1):\n",
    "    count_vectorizer = CountVectorizer(stop_words={'english'}, ngram_range=(1,1), max_df=i/10)\n",
    "    X_train_vectorized = count_vectorizer.fit_transform(X_train)\n",
    "    print(f'This is the shape for {i/10}')\n",
    "    print(X_train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c38a720",
   "metadata": {},
   "source": [
    "I don't see much of a difference in this max_df, even at really low percentages. But, never know if this can really affect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "851aa8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>character</td>\n",
       "      <td>4746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>films</td>\n",
       "      <td>4627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>life</td>\n",
       "      <td>4399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>where</td>\n",
       "      <td>4350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>plot</td>\n",
       "      <td>4349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>little</td>\n",
       "      <td>4309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>show</td>\n",
       "      <td>4296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>love</td>\n",
       "      <td>4287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>over</td>\n",
       "      <td>4270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>best</td>\n",
       "      <td>4199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>did</td>\n",
       "      <td>4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>know</td>\n",
       "      <td>4167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>does</td>\n",
       "      <td>4065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>off</td>\n",
       "      <td>4059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ever</td>\n",
       "      <td>3982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>man</td>\n",
       "      <td>3980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>better</td>\n",
       "      <td>3936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>here</td>\n",
       "      <td>3884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>end</td>\n",
       "      <td>3807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>your</td>\n",
       "      <td>3794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  counts\n",
       "0   character    4746\n",
       "1       films    4627\n",
       "2        life    4399\n",
       "3       where    4350\n",
       "4        plot    4349\n",
       "5      little    4309\n",
       "6        show    4296\n",
       "7        love    4287\n",
       "8        over    4270\n",
       "9        best    4199\n",
       "10        did    4190\n",
       "11       know    4167\n",
       "12       does    4065\n",
       "13        off    4059\n",
       "14       ever    3982\n",
       "15        man    3980\n",
       "16     better    3936\n",
       "17       here    3884\n",
       "18        end    3807\n",
       "19       your    3794"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Most common words in general\n",
    "\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,1), max_df=.2)\n",
    "list_of_words = cv.fit_transform(X_train).toarray()\n",
    "sum_of_words = pd.Series(list_of_words.sum(axis=0))\n",
    "sum_of_words = sum_of_words.sort_values(ascending=False)[:20]\n",
    "\n",
    "words = []\n",
    "numbers = []\n",
    "\n",
    "for num in sum_of_words.index:\n",
    "    word = (list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(num)])\n",
    "    numbers.append(sum_of_words.loc[num])\n",
    "    words.append(word)\n",
    "    \n",
    "pd.DataFrame({\n",
    "    \"words\" : words,\n",
    "    \"counts\" : numbers,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8c2100",
   "metadata": {},
   "source": [
    "#### Model number 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca7d72fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 3,718,004 \n",
      "The amount of time it took to vectorize was: 18.143799\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3751  386]\n",
      " [ 530 3583]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      4137\n",
      "           1       0.90      0.87      0.89      4113\n",
      "\n",
      "    accuracy                           0.89      8250\n",
      "   macro avg       0.89      0.89      0.89      8250\n",
      "weighted avg       0.89      0.89      0.89      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.9027462836986646\n",
      "Accuracy Score: 0.888969696969697\n",
      "Recall Score: 0.8711402868952103\n",
      "f1 Score 0.886661717396684\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), max_df=.2 )\n",
    "\n",
    "for cv ,clf, clf_name in [(vec , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(vec, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "803f4ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 3,718,004 \n",
      "The amount of time it took to vectorize was: 17.942894000000024\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3751  386]\n",
      " [ 530 3583]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      4137\n",
      "           1       0.90      0.87      0.89      4113\n",
      "\n",
      "    accuracy                           0.89      8250\n",
      "   macro avg       0.89      0.89      0.89      8250\n",
      "weighted avg       0.89      0.89      0.89      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.9027462836986646\n",
      "Accuracy Score: 0.888969696969697\n",
      "Recall Score: 0.8711402868952103\n",
      "f1 Score 0.886661717396684\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), max_df=.3)\n",
    "\n",
    "for cv ,clf, clf_name in [(vec , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(vec, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b73dd",
   "metadata": {},
   "source": [
    "This models perform quiet similar to the models that where run with the decided stop_words (which was expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e3cd0d",
   "metadata": {},
   "source": [
    "#### Model number 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ddfe39c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 3,718,004 \n",
      "The amount of time it took to vectorize was: 18.375955000000033\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3751  386]\n",
      " [ 530 3583]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      4137\n",
      "           1       0.90      0.87      0.89      4113\n",
      "\n",
      "    accuracy                           0.89      8250\n",
      "   macro avg       0.89      0.89      0.89      8250\n",
      "weighted avg       0.89      0.89      0.89      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.9027462836986646\n",
      "Accuracy Score: 0.888969696969697\n",
      "Recall Score: 0.8711402868952103\n",
      "f1 Score 0.886661717396684\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "stop_words = 'the and of to is in br it that this as with for was film movie his on are have be one'.split(' ')\n",
    "cv = CountVectorizer(stop_words=stop_words, ngram_range=(1,3), max_df=.3)\n",
    "\n",
    "\n",
    "for cv ,clf, clf_name in [(vec , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(vec, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34387d",
   "metadata": {},
   "source": [
    "There was a lot of overlap, and this little amount of words really make a difference in the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f83cc",
   "metadata": {},
   "source": [
    "#### Models using min_df alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75874e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the shape for 2\n",
      "(16750, 38025)\n",
      "This is the shape for 5\n",
      "(16750, 22483)\n",
      "This is the shape for 10\n",
      "(16750, 14804)\n",
      "This is the shape for 15\n",
      "(16750, 11432)\n",
      "This is the shape for 20\n",
      "(16750, 9375)\n",
      "This is the shape for 25\n",
      "(16750, 7969)\n",
      "This is the shape for 30\n",
      "(16750, 7034)\n",
      "This is the shape for 50\n",
      "(16750, 4803)\n"
     ]
    }
   ],
   "source": [
    "for i in [2,5,10,15,20,25,30,50]:\n",
    "    count_vectorizer = CountVectorizer(stop_words={'english'}, ngram_range=(1,1), min_df=i)\n",
    "    X_train_vectorized = count_vectorizer.fit_transform(X_train)\n",
    "    print(f'This is the shape for {i}')\n",
    "    print(X_train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c819647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the shape for 2\n",
      "(16750, 622962)\n",
      "This is the shape for 5\n",
      "(16750, 175106)\n",
      "This is the shape for 10\n",
      "(16750, 80501)\n",
      "This is the shape for 15\n",
      "(16750, 52307)\n",
      "This is the shape for 20\n",
      "(16750, 38496)\n",
      "This is the shape for 25\n",
      "(16750, 30253)\n",
      "This is the shape for 30\n",
      "(16750, 24837)\n",
      "This is the shape for 50\n",
      "(16750, 14359)\n"
     ]
    }
   ],
   "source": [
    "for i in [2,5,10,15,20,25,30,50]:\n",
    "    count_vectorizer = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), min_df=i)\n",
    "    X_train_vectorized = count_vectorizer.fit_transform(X_train)\n",
    "    print(f'This is the shape for {i}')\n",
    "    print(X_train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d8c3cff7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the shape for 2\n",
      "(16750, 408279)\n",
      "This is the shape for 5\n",
      "(16750, 99325)\n",
      "This is the shape for 10\n",
      "(16750, 43980)\n",
      "This is the shape for 15\n",
      "(16750, 28243)\n",
      "This is the shape for 20\n",
      "(16750, 20688)\n",
      "This is the shape for 25\n",
      "(16750, 16255)\n",
      "This is the shape for 30\n",
      "(16750, 13391)\n",
      "This is the shape for 50\n",
      "(16750, 7828)\n"
     ]
    }
   ],
   "source": [
    "for i in [2,5,10,15,20,25,30,50]:\n",
    "    count_vectorizer = CountVectorizer(stop_words=stop_words, ngram_range=(1,3), min_df=i)\n",
    "    X_train_vectorized = count_vectorizer.fit_transform(X_train)\n",
    "    print(f'This is the shape for {i}')\n",
    "    print(X_train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6e0b08",
   "metadata": {},
   "source": [
    "#### Model number 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28c5b373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 99,325 \n",
      "The amount of time it took to vectorize was: 11.592953999999963\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3639  498]\n",
      " [ 505 3608]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      4137\n",
      "           1       0.88      0.88      0.88      4113\n",
      "\n",
      "    accuracy                           0.88      8250\n",
      "   macro avg       0.88      0.88      0.88      8250\n",
      "weighted avg       0.88      0.88      0.88      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.8787140769605456\n",
      "Accuracy Score: 0.8784242424242424\n",
      "Recall Score: 0.8772185752492099\n",
      "f1 Score 0.8779656892566006\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words=stop_words, ngram_range=(1,3), min_df=5)\n",
    "\n",
    "\n",
    "for cv ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(cv, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5048d13",
   "metadata": {},
   "source": [
    "#### Model number 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19a7983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 43,980 \n",
      "The amount of time it took to vectorize was: 10.890859999999975\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3592  545]\n",
      " [ 510 3603]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87      4137\n",
      "           1       0.87      0.88      0.87      4113\n",
      "\n",
      "    accuracy                           0.87      8250\n",
      "   macro avg       0.87      0.87      0.87      8250\n",
      "weighted avg       0.87      0.87      0.87      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.8686113789778206\n",
      "Accuracy Score: 0.8721212121212121\n",
      "Recall Score: 0.8760029175784099\n",
      "f1 Score 0.8722914901343661\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words=stop_words, ngram_range=(1,3), min_df=10)\n",
    "\n",
    "\n",
    "for cv ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(cv, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748d255",
   "metadata": {},
   "source": [
    "Lets try some mixture of both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca693001",
   "metadata": {},
   "source": [
    "#### Model number 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3e6c60c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 80,433 \n",
      "The amount of time it took to vectorize was: 13.848969000000011\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3630  507]\n",
      " [ 445 3668]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88      4137\n",
      "           1       0.88      0.89      0.89      4113\n",
      "\n",
      "    accuracy                           0.88      8250\n",
      "   macro avg       0.88      0.88      0.88      8250\n",
      "weighted avg       0.88      0.88      0.88      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.878562874251497\n",
      "Accuracy Score: 0.8846060606060606\n",
      "Recall Score: 0.8918064672988086\n",
      "f1 Score 0.8851351351351352\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), min_df=10, max_df=0.3)\n",
    "\n",
    "\n",
    "for cv ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(cv, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d2f446",
   "metadata": {},
   "source": [
    "#### Model number 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4b09b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 80,457 \n",
      "The amount of time it took to vectorize was: 13.599285000000009\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3628  509]\n",
      " [ 449 3664]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88      4137\n",
      "           1       0.88      0.89      0.88      4113\n",
      "\n",
      "    accuracy                           0.88      8250\n",
      "   macro avg       0.88      0.88      0.88      8250\n",
      "weighted avg       0.88      0.88      0.88      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.8780254013898874\n",
      "Accuracy Score: 0.8838787878787879\n",
      "Recall Score: 0.8908339411621687\n",
      "f1 Score 0.8843832971276853\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), min_df=10, max_df=0.4)\n",
    "\n",
    "\n",
    "for cv ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(cv, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb10688",
   "metadata": {},
   "source": [
    "The combination of both doesn't look better (need more testing to check if it is the same) but it may be faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c1dde",
   "metadata": {},
   "source": [
    "#### Model number 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5db7ebc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 175,062 \n",
      "The amount of time it took to vectorize was: 13.904557000000068\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3661  476]\n",
      " [ 474 3639]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89      4137\n",
      "           1       0.88      0.88      0.88      4113\n",
      "\n",
      "    accuracy                           0.88      8250\n",
      "   macro avg       0.88      0.88      0.88      8250\n",
      "weighted avg       0.88      0.88      0.88      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.884325637910085\n",
      "Accuracy Score: 0.8848484848484849\n",
      "Recall Score: 0.8847556528081693\n",
      "f1 Score 0.8845405930967428\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), min_df=5, max_df=0.4)\n",
    "\n",
    "\n",
    "for cv ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(cv, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1301727",
   "metadata": {},
   "source": [
    "#### Model number 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7af35965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 622,918 \n",
      "The amount of time it took to vectorize was: 15.014494000000013\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3704  433]\n",
      " [ 467 3646]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89      4137\n",
      "           1       0.89      0.89      0.89      4113\n",
      "\n",
      "    accuracy                           0.89      8250\n",
      "   macro avg       0.89      0.89      0.89      8250\n",
      "weighted avg       0.89      0.89      0.89      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.8938465310125031\n",
      "Accuracy Score: 0.8909090909090909\n",
      "Recall Score: 0.886457573547289\n",
      "f1 Score 0.89013671875\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), min_df=2, max_df=0.4)\n",
    "\n",
    "\n",
    "for cv ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(cv, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530642cd",
   "metadata": {},
   "source": [
    "I would like to understand exactly why when you start to increase the min_df you get a worse score if also you are working with a max_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959686cc",
   "metadata": {},
   "source": [
    "### 3.5) Tailored based stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cc68b",
   "metadata": {},
   "source": [
    "### 4) Contrasting Models and A/B testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0b606db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie is another Christian propaganda fil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A woman who hates cats (Alice Krige) and her s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beast Wars is a show that is over-hyped, overp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An excellent example of \"cowboy noir\", as it's...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ok, basically this is a popcorn sci-fi movie, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>Jimmy Cagney races by your eyes constantly in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>Very much a film from the times -- extremely l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>The Little Mermaid is one of my absolute favor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>With a simplistic story and an engaging heroin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>The 63 year reign of Queen Victoria is perhaps...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                contents  labels\n",
       "0      This movie is another Christian propaganda fil...       0\n",
       "1      A woman who hates cats (Alice Krige) and her s...       1\n",
       "2      Beast Wars is a show that is over-hyped, overp...       0\n",
       "3      An excellent example of \"cowboy noir\", as it's...       1\n",
       "4      Ok, basically this is a popcorn sci-fi movie, ...       1\n",
       "...                                                  ...     ...\n",
       "24995  Jimmy Cagney races by your eyes constantly in ...       1\n",
       "24996  Very much a film from the times -- extremely l...       0\n",
       "24997  The Little Mermaid is one of my absolute favor...       0\n",
       "24998  With a simplistic story and an engaging heroin...       1\n",
       "24999  The 63 year reign of Queen Victoria is perhaps...       1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83f69a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "for i1,i2 in kf.split(data):\n",
    "    lst1 = list(i1)\n",
    "    lst2 = list(i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa6687d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), min_df=2, max_df=0.4)\n",
    "X = 1\n",
    "y = 1\n",
    "test_split = 1\n",
    "\n",
    "def model_run(cv, clf, model_name, X, y, test_split):\n",
    "    for cv ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "        # Vectorization\n",
    "        start = time.process_time()\n",
    "        X_train_vectorized, X_test_vectorized = vectorize_X(cv, X_train, X_test)\n",
    "        print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "        print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "\n",
    "        # Running the model\n",
    "        print(clf_name)\n",
    "        clf.fit(X_train_vectorized, y_train)\n",
    "        check_model(clf, X_test_vectorized, y_test)\n",
    "        print(\"_________________________________________________\")\n",
    "        \n",
    "def get_test_split():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "014b6dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12500</th>\n",
       "      <td>argh! this film hurts my head. and not in a go...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12501</th>\n",
       "      <td>Istanbul is another one of those expatriate fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12502</th>\n",
       "      <td>As a history nut who is particularly intereste...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12503</th>\n",
       "      <td>This is a very enjoyable film with excellent a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12504</th>\n",
       "      <td>Me being of Irish origins, loved this movie, N...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>Jimmy Cagney races by your eyes constantly in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>Very much a film from the times -- extremely l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>The Little Mermaid is one of my absolute favor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>With a simplistic story and an engaging heroin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>The 63 year reign of Queen Victoria is perhaps...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                contents  labels\n",
       "12500  argh! this film hurts my head. and not in a go...       0\n",
       "12501  Istanbul is another one of those expatriate fi...       0\n",
       "12502  As a history nut who is particularly intereste...       0\n",
       "12503  This is a very enjoyable film with excellent a...       1\n",
       "12504  Me being of Irish origins, loved this movie, N...       1\n",
       "...                                                  ...     ...\n",
       "24995  Jimmy Cagney races by your eyes constantly in ...       1\n",
       "24996  Very much a film from the times -- extremely l...       0\n",
       "24997  The Little Mermaid is one of my absolute favor...       0\n",
       "24998  With a simplistic story and an engaging heroin...       1\n",
       "24999  The 63 year reign of Queen Victoria is perhaps...       1\n",
       "\n",
       "[12500 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[~data.index.isin(i1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
