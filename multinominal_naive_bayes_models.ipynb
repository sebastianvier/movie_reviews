{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7cd9b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python in build modules:\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "## EDA libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "## Metrics (sklearn)\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "\n",
    "## Models (sklearn)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ec5ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(y_true, prediction):\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix(y_true, prediction))\n",
    "    print(\"\\n\")\n",
    "    print('Classification Report')\n",
    "    print(classification_report(y_true, prediction))\n",
    "    print(\"\\n\")\n",
    "    print(\"Other Metrics:\")\n",
    "    print(f'Pression Score: {precision_score(y_true, prediction)}')\n",
    "    print(f'Accuracy Score: {accuracy_score(y_true, prediction)}')\n",
    "    print(f'Recall Score: {recall_score(y_true, prediction)}')\n",
    "    print(f'f1 Score {f1_score(y_true, prediction)}')\n",
    "    \n",
    "def check_model(clf, X_test, y_test):\n",
    "    prediction = clf.predict(X_test)\n",
    "    print_report(y_test, prediction)\n",
    "    \n",
    "def vectorize_X(vectorizer, X_train, X_test):\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized  = vectorizer.transform(X_test)\n",
    "    return X_train_vectorized, X_test_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95ef8b2",
   "metadata": {},
   "source": [
    "### 1) Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd00805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "## Importing the data\n",
    "\n",
    "path = 'data/train/'\n",
    "\n",
    "count = 0\n",
    "labels = []\n",
    "contents = []\n",
    "\n",
    "for label in ['neg','pos']:\n",
    "    filenames = os.listdir(path + label)\n",
    "    for filename in filenames:\n",
    "        count += 1\n",
    "        with open(os.path.join(path, label, filename), 'r') as f:\n",
    "            labels.append(1 if label == 'pos' else 0) # 1 is positve 0 is negative\n",
    "            contents.append(f.read())\n",
    "print(count)\n",
    "            \n",
    "data = pd.DataFrame({\n",
    "    'contents' : contents,\n",
    "    'labels': labels,\n",
    "\n",
    "})\n",
    "\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True) # This code will shuffle the data (just in case!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdecdb1c",
   "metadata": {},
   "source": [
    "#### Just to be clear negative is 0 and positive is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1935c7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12500\n",
       "1    12500\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab00242",
   "metadata": {},
   "source": [
    "Ok, the data is **completely balanced**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298663a9",
   "metadata": {},
   "source": [
    " ###### At this moment we are going to see 5 examples of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65b3dd8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "\n",
      "\n",
      "I read some comments on the internet about this film like \"...harder then Hostel...\", \"the camera never screens of when it's getting really brutal...\". But none of them is true. The camera never screens of, because there is nothing to screen of. The same scene is repeated hundred and hundred times again. Women lies on a table, killer rapes women a few times, killer cuts women into pieces (you never see this during the whole film!). Police come and arrested him. Killer fools the jury. Film over. In Germany we would say :\"Viel LÃ¤rm um Nichts\". All in all, one of the most boring films I ever see. Absolutely non-recommendable.\n",
      "\n",
      "\n",
      "It really is that bad of a movie. My buddy rented it because he, well, is an idiot. But then again, I must be an idiot too because I watched the whole damn thing! The actors were on par with high school drama geeks who think that are going places. The only place they will be going is back to waiting tables at Luby's. All I could think of while I was watching this \"gem\" was how it actually got made. I mean, some \"screenwriter\" actually thought that this premise was fresh, original and lucrative. Then some moron with money believed in the script so much that he decided to fork some cash over with the naive misconception that he was going to make a return on it. Actors were cast, locations were scouted, make-up artists were hired, computer animators fresh out of Al Collins graphic Design School were brought in and this turd started to take form.<br /><br />There obviously were a ton of things that I hated about this move but the one thing the drove me the craziest was the overuse of music. Every single minute of this flick was scored. There was not a single break in music. And at times it was mixed higher than the dialogue, not that it made you miss some vital plot point or anything.<br /><br />After it was over, we decided to watch Mystic River. It was like driving a 1980 VW Diesel Rabbit then switching to a BMW 740il. You couldn't get two more opposite movies in terms of quality.\n",
      "\n",
      "\n",
      "This is simply the worst movie I've ever seen. Neither of the three central characters has any charm, and Erika's good looks aren't enough to carry the film. The lamest plot I've ever had inflicted upon me. Also the most unconvincing military comedy ever. Why did they bother?\n",
      "\n",
      "\n",
      "Matt Cordell is back from the dead for a third go-round, although I'm not sure anyone cared at this point except for rabid MANICA COP fans. Cordell, who died in the last flick, is resurrected through voodoo, and is now hot on the trail of several miscreants involved in the shooting of a fellow officer Cordell is very fond of. I missed part of this early '90s low-budget quickie, but it was pleasing to see Cordell wracking up the body count in various, gruesome ways. Problem is, the overall film is pretty static, and Cordell simply ain't Jason or Freddy. The interest wanes pretty fast, even with that grand B-movie master Robert Forster as a doctor who ends up with his brains scrambled. Stick with the first film in the series, which is funny and scary and exciting, all at the same time.\n",
      "\n",
      "\n",
      "I could barely keep myself from either nodding off or just turning off this turd, but I decided to stick it out if only for the reasoning that maybe *something* would happen. This is the work of a writer/producer/director/special fx, Kenneth Herts, who wants to make a statement on ecological damage while making a monster movie. That's what he wanted, anyway. What it turns out to be is a lot of acting, either slightly hammy or just mundane and without much merit, and scenes that seem to repeat themselves as the monster ATTACKS in the river waters (oh, and what luck, a woman just happens to be naked in it... even though there have already been DISAPPEARANCES!) <br /><br />This is just nonsensical stuff, but I suppose it's not too harmful; it's not very obnoxious at the least and once or twice we get a semi-interesting peek at Brazilian \"culture\" (which is the father walking through town with his flock or other pieces of a semblance of 'hey, this is NOT America!'). But whatever hope the director had in casting Mitchum or Carradine is squandered on at best pedestrian and at worst excruciatingly banal and dumb dialog. It doesn't help that when we finally get something of a good look at the monster and the \"action\" happens, it too is stupidly staged and with only sleazy appeal. Usually I would feel sorry for a filmmaker who had a lot of problems getting a particular picture finished- in this case it took the better part of the mid 70s- but with Monstroid or Monster or whatever it's called... nah.<br /><br />If you happen to get the Elvira DVD double-feature of this (bad print with bad transfer quality) with Blue Sunshine, make sure to skip this one. Unless, of course, you're an Elvira die-hard and can't help yourself to hear her luscious commentary; personally, I'd rather get Joel or Mike Nelson with the robots from Mystery Science Theater on this roast turkey.\n",
      "\n",
      "\n",
      "positive\n",
      "\n",
      "\n",
      "If you are looking for a sonic-boom-special-effects monster, click the BACK button on your browser.<br /><br />Deathtrap was written by Ira Levin (Sliver, The Stepford Wives, Rosemary's Baby). It's a stage play, adapted for the screen. 95% of the movie takes place in the gorgeous home of playwright Sidney Bruhl (Michael Caine). He's the author of a fabulously successful Broadway play, but his last 4 efforts have flopped - horribly.<br /><br />An aspiring playwright, Clifford Anderson (Christopher Reeve), who attended a play-writing workshop given by Sydney, has sent him a copy of the play he has written. Sydney tells his wife, Myra (Dyan Cannon) the play is fabulous - a sure-fire hit. But is it good enough to die for? Time will tell.<br /><br />Clever dialog and numerous twists and turns in the plot keep this movie entertaining from beginning to end. The whole cast seems to have a good time. It's reminiscent of another fun Michael Caine mystery: Sleuth. Worth watching.<br /><br />\n",
      "\n",
      "\n",
      "Bruce Almighty, one of Carrey's best pictures since... well... a long time. It contains one of the funniest scenes I have seen for a long time too... Morgan Freeman plays God well and even chips in a few jokes that are surprisingly funny. It contains one or two romantic moments that are a bit boring but over all a great movie with some funny scenes. The best scene in, it is where Jim is messing up the anchor man's voice.<br /><br />My rating: 8/10\n",
      "\n",
      "\n",
      "It is difficult to evaluate this or any other comparable film of the early sound era in terms that one might use for ordinary film commentary. At times there is almost a desperation, as many film personalities of the silent era try their wings at sound, surely fearing that they will be left by the wayside (as did happen to some), Rin-Tin-Tin. however, was pertfectly natural. In such a vaudeville of unrelated sequences, some were sure to stand out John Barrymore's soliloquy from Richard II is a moment certainly worth preserving. By and large, only those with earlier stage training exuded confidence. However, this is over all reasonably entertaining, and a must for \"film buffs\" especially interested in the silent to sound transition\n",
      "\n",
      "\n",
      "I can't say whether the post-WWII British comedies produced at the Ealing Studios are an acquired taste or not, but I am completely addicted, and The Man in the White Suit is one of the best. No need to go into the well-known plot about the threat posed to both the textile industry and the textile unions by an indestructible, dirt-resistant fiber. Suffice it to say that the slings and arrows suffered by the naively idealistic Sidney Stratton in pursuing his polymer vision make for a comedic delight. Many of the well-known faces from the world of British character actors - the nervous Cecil Parker, the suavely devious Michael Gough, and the bluntly ruthless Ernest Thesinger - put in wonderful performances. Guinness - as always and forever - is superb, and Joan Greenwood is delectable as Daphne (just the way she enunciates the word \"Daddy\", makes the entire movie worth seeing).<br /><br />\"Knudsen!!!!!!!\"\n",
      "\n",
      "\n",
      "\"The Bourne Ultimatum\" begins recklessly mid-chase and in pulse-pounding fashion explodes from there as Jason Bourne (Matt Damon, absolutely superb) tracks down the masterminds behind the CIA black-ops that turned him into the perfect killer in a final attempt to learn his true identity. A devastatingly icy David Strathairn as the \"man behind the curtain\" is added to the returning cast of regulars including Joan Allen (excellent) and Julia Stiles (non-existent).<br /><br />Like the second entry in the series, I wished Paul Greengrass' shaky hand-held camera would go static at least for the few minutes of downtime. However, that being said, it's a perfect way to capture the tense, claustrophobic feel of the intimate hand-to-hand-combat scenes and works equally well in the chase scenes which are mostly on foot and across rooftops with the occasional big car pile-up. Part of the fun of the Bourne series is the constant globe-hopping and manipulation of technology and communications that seem to defy the laws of physics and current capabilities. The Bourne films seem to exist in some sort of gritty hyper-reality that is full of technological-based magic. It makes no sense that everyone seems to be just in the right place at the right time, but I'll be damned if it isn't a blast to watch them get there.<br /><br />With the absence of the emotive and involving Franka Potente, the writers attempt to create some emotional connection between Damon and Stiles, but she is so blank-faced an actress it never really leads to anything. Still, this can be forgiven, for unlike the \"Identity\" and the \"Supremacy\", this \"Ultimatum\" reveals all and we finally learn the truth about Bourne's past. It's an entertaining and satisfying conclusion to the series, and if they have any good sense, and Damon gets his wish, this will be the perfect end to it.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for label in data.labels.unique():\n",
    "    print(\"negative\" if label ==  0 else \"positive\")\n",
    "    print(\"\\n\")\n",
    "    for content in data.contents[data.labels == label].sample(5):\n",
    "        print(content)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd2f5d0",
   "metadata": {},
   "source": [
    "###### From now on I am going to be using the training data to explore the data so I can get some conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99e48b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.contents\n",
    "y = data.labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54397f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_train, y_train],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edfbbc31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8387\n",
       "0    8363\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.labels.value_counts() # Just checking the proportions haven't change much"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c034f50c",
   "metadata": {},
   "source": [
    "###### Mean of length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfb077e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of all reviews: 1328.28\n",
      "Mean length in positive reviews: 1352.3945391677596\n",
      "Mean length in negative reviews: 1304.1033122085375\n",
      "Ratio positive: 1.0181547107294844\n",
      "Ration negative: 0.9817985004731966\n"
     ]
    }
   ],
   "source": [
    "mean_length_full = np.round(X_train.map(len).mean(),2)\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "mean_lenght_pos = train_df[train_df.labels == 1].contents.map(len).mean()\n",
    "mean_length_neg = train_df[train_df.labels == 0].contents.map(len).mean()\n",
    "ratio_pos =  mean_lenght_pos / mean_length_full\n",
    "ratio_neg =  mean_length_neg / mean_length_full\n",
    "\n",
    "print(f'Mean length of all reviews: {mean_length_full}')\n",
    "print(f'Mean length in positive reviews: {mean_lenght_pos}')\n",
    "print(f'Mean length in negative reviews: {mean_length_neg}')\n",
    "print(f'Ratio positive: {ratio_pos}')\n",
    "print(f'Ration negative: {ratio_neg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd11a0",
   "metadata": {},
   "source": [
    "We can run an **A/B test** here, but there is an indication that the **length** of the commentary has **nothing to do** with the idea if it is positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587a175",
   "metadata": {},
   "source": [
    "###### Describing of length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48c84568",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Describe the length of model when positive:\n",
      "count     8387.000000\n",
      "mean      1352.394539\n",
      "std       1053.673512\n",
      "min         70.000000\n",
      "25%        695.000000\n",
      "50%        982.000000\n",
      "75%       1659.500000\n",
      "max      13704.000000\n",
      "Name: contents, dtype: float64\n",
      "\n",
      "\n",
      "Describe the length of model when negative:\n",
      "count    8363.000000\n",
      "mean     1304.103312\n",
      "std       963.063593\n",
      "min        53.000000\n",
      "25%       710.000000\n",
      "50%       981.000000\n",
      "75%      1554.000000\n",
      "max      8754.000000\n",
      "Name: contents, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Describe the length of model when positive:')\n",
    "print(train_df[train_df.labels == 1].contents.map(len).describe())\n",
    "print('\\n')\n",
    "print('Describe the length of model when negative:')\n",
    "print(train_df[train_df.labels == 0].contents.map(len).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eea6ef41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A woman who hates cats (Alice Krige) and her son (Brian Krause) have moved into a small town, and must deal with a mean teacher (Glenn Shadix), their incestuous relationship, a lovely girl (MÃ¤dchen Amick) and one hell of a big secret.<br /><br />Okay, so technically, this is a \"bad film\". But, who cares? It\\'s so very fun! <br /><br />Impossible things (involving corn) happen, people freak out about kitty cats, there\\'s bad one-liners, there\\'s too much cheese to handle!<br /><br />So, yes. You will enjoy this. A lot. It won\\'t move you, touch you, scare you, or thrill you in any way, but it will keep you entertained and laughing!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[1].contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7357e44b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a3ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most common words in general\n",
    "\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(3,3))\n",
    "list_of_words = cv.fit_transform(X_train).toarray()\n",
    "sum_of_words = pd.Series(list_of_words.sum(axis=0))\n",
    "sum_of_words = sum_of_words.sort_values(ascending=False)[:20]\n",
    "\n",
    "words = []\n",
    "numbers = []\n",
    "\n",
    "for num in sum_of_words.index:\n",
    "    word = (list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(num)])\n",
    "    numbers.append(sum_of_words.loc[num])\n",
    "    words.append(word)\n",
    "    \n",
    "pd.DataFrame({\n",
    "    \"words\" : words,\n",
    "    \"counts\" : numbers,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58751a6",
   "metadata": {},
   "source": [
    "###### Most common word for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d57b119",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>226374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>110239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>97692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>91327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>72318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>br</td>\n",
       "      <td>68495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>it</td>\n",
       "      <td>65011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td>62845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>this</td>\n",
       "      <td>50974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>that</td>\n",
       "      <td>49116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>was</td>\n",
       "      <td>32075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>as</td>\n",
       "      <td>31526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>for</td>\n",
       "      <td>29811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>movie</td>\n",
       "      <td>29732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>with</td>\n",
       "      <td>29532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>but</td>\n",
       "      <td>28659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>film</td>\n",
       "      <td>26875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>you</td>\n",
       "      <td>23086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>on</td>\n",
       "      <td>22838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>not</td>\n",
       "      <td>20462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words  counts\n",
       "0     the  226374\n",
       "1     and  110239\n",
       "2      of   97692\n",
       "3      to   91327\n",
       "4      is   72318\n",
       "5      br   68495\n",
       "6      it   65011\n",
       "7      in   62845\n",
       "8    this   50974\n",
       "9    that   49116\n",
       "10    was   32075\n",
       "11     as   31526\n",
       "12    for   29811\n",
       "13  movie   29732\n",
       "14   with   29532\n",
       "15    but   28659\n",
       "16   film   26875\n",
       "17    you   23086\n",
       "18     on   22838\n",
       "19    not   20462"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Most common words in general\n",
    "\n",
    "cv = CountVectorizer(stop_words={'english'})\n",
    "list_of_words = cv.fit_transform(X_train).toarray()\n",
    "sum_of_words = pd.Series(list_of_words.sum(axis=0))\n",
    "sum_of_words = sum_of_words.sort_values(ascending=False)[:20]\n",
    "\n",
    "words = []\n",
    "numbers = []\n",
    "\n",
    "for num in sum_of_words.index:\n",
    "    word = (list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(num)])\n",
    "    numbers.append(sum_of_words.loc[num])\n",
    "    words.append(word)\n",
    "    \n",
    "pd.DataFrame({\n",
    "    \"words\" : words,\n",
    "    \"counts\" : numbers,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e007398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>116902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>60296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>51458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>45150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>38653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in</td>\n",
       "      <td>33731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>br</td>\n",
       "      <td>33091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>it</td>\n",
       "      <td>32486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>that</td>\n",
       "      <td>23998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this</td>\n",
       "      <td>23519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as</td>\n",
       "      <td>17752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>with</td>\n",
       "      <td>15599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>for</td>\n",
       "      <td>15132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>was</td>\n",
       "      <td>14698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>film</td>\n",
       "      <td>14146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>but</td>\n",
       "      <td>14098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>movie</td>\n",
       "      <td>12845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>his</td>\n",
       "      <td>11603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>on</td>\n",
       "      <td>11422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>you</td>\n",
       "      <td>11294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words  counts\n",
       "0     the  116902\n",
       "1     and   60296\n",
       "2      of   51458\n",
       "3      to   45150\n",
       "4      is   38653\n",
       "5      in   33731\n",
       "6      br   33091\n",
       "7      it   32486\n",
       "8    that   23998\n",
       "9    this   23519\n",
       "10     as   17752\n",
       "11   with   15599\n",
       "12    for   15132\n",
       "13    was   14698\n",
       "14   film   14146\n",
       "15    but   14098\n",
       "16  movie   12845\n",
       "17    his   11603\n",
       "18     on   11422\n",
       "19    you   11294"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_X_train = train_df.contents[train_df.labels == 1]\n",
    "\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,1))\n",
    "list_of_words = cv.fit_transform(pos_X_train).toarray()\n",
    "sum_of_words = pd.Series(list_of_words.sum(axis=0))\n",
    "sum_of_words = sum_of_words.sort_values(ascending=False)[:20]\n",
    "\n",
    "words = []\n",
    "numbers = []\n",
    "\n",
    "for num in sum_of_words.index:\n",
    "    word = (list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(num)])\n",
    "    numbers.append(sum_of_words.loc[num])\n",
    "    words.append(word)\n",
    "    \n",
    "pd.DataFrame({\n",
    "    \"words\" : words,\n",
    "    \"counts\" : numbers,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0736ff54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>116902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>60296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>51458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>45150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>38653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in</td>\n",
       "      <td>33731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>br</td>\n",
       "      <td>33091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>it</td>\n",
       "      <td>32486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>that</td>\n",
       "      <td>23998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this</td>\n",
       "      <td>23519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as</td>\n",
       "      <td>17752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>with</td>\n",
       "      <td>15599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>for</td>\n",
       "      <td>15132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>was</td>\n",
       "      <td>14698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>film</td>\n",
       "      <td>14146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>but</td>\n",
       "      <td>14098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>movie</td>\n",
       "      <td>12845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>his</td>\n",
       "      <td>11603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>on</td>\n",
       "      <td>11422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>you</td>\n",
       "      <td>11294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words  counts\n",
       "0     the  116902\n",
       "1     and   60296\n",
       "2      of   51458\n",
       "3      to   45150\n",
       "4      is   38653\n",
       "5      in   33731\n",
       "6      br   33091\n",
       "7      it   32486\n",
       "8    that   23998\n",
       "9    this   23519\n",
       "10     as   17752\n",
       "11   with   15599\n",
       "12    for   15132\n",
       "13    was   14698\n",
       "14   film   14146\n",
       "15    but   14098\n",
       "16  movie   12845\n",
       "17    his   11603\n",
       "18     on   11422\n",
       "19    you   11294"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_X_train = train_df.contents[train_df.labels == 0]\n",
    "\n",
    "cv = CountVectorizer(stop_words={'english'})\n",
    "list_of_words = cv.fit_transform(pos_X_train).toarray()\n",
    "sum_of_words = pd.Series(list_of_words.sum(axis=0))\n",
    "sum_of_words = sum_of_words.sort_values(ascending=False)[:20]\n",
    "\n",
    "words = []\n",
    "numbers = []\n",
    "\n",
    "for num in sum_of_words.index:\n",
    "    word = (list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(num)])\n",
    "    numbers.append(sum_of_words.loc[num])\n",
    "    words.append(word)\n",
    "    \n",
    "pd.DataFrame({\n",
    "    \"words\" : words,\n",
    "    \"counts\" : numbers,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75815e",
   "metadata": {},
   "source": [
    "#### 2) Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3e56711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>but</td>\n",
       "      <td>28659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you</td>\n",
       "      <td>23086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>not</td>\n",
       "      <td>20462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>he</td>\n",
       "      <td>20282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>all</td>\n",
       "      <td>16114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>at</td>\n",
       "      <td>15768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>they</td>\n",
       "      <td>15458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>by</td>\n",
       "      <td>15087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>an</td>\n",
       "      <td>14403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>who</td>\n",
       "      <td>14306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>so</td>\n",
       "      <td>13948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>from</td>\n",
       "      <td>13817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>like</td>\n",
       "      <td>13521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>there</td>\n",
       "      <td>12722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>her</td>\n",
       "      <td>12454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>or</td>\n",
       "      <td>12129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>just</td>\n",
       "      <td>11926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>about</td>\n",
       "      <td>11818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>out</td>\n",
       "      <td>11523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>if</td>\n",
       "      <td>11301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    words  counts\n",
       "0     but   28659\n",
       "1     you   23086\n",
       "2     not   20462\n",
       "3      he   20282\n",
       "4     all   16114\n",
       "5      at   15768\n",
       "6    they   15458\n",
       "7      by   15087\n",
       "8      an   14403\n",
       "9     who   14306\n",
       "10     so   13948\n",
       "11   from   13817\n",
       "12   like   13521\n",
       "13  there   12722\n",
       "14    her   12454\n",
       "15     or   12129\n",
       "16   just   11926\n",
       "17  about   11818\n",
       "18    out   11523\n",
       "19     if   11301"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = 'the and of to is in br it that this as with for was film movie his on are have be one'.split(' ')\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "list_of_words = cv.fit_transform(X_train).toarray()\n",
    "sum_of_words = pd.Series(list_of_words.sum(axis=0))\n",
    "sum_of_words = sum_of_words.sort_values(ascending=False)[:20]\n",
    "\n",
    "words = []\n",
    "numbers = []\n",
    "\n",
    "for num in sum_of_words.index:\n",
    "    word = (list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(num)])\n",
    "    numbers.append(sum_of_words.loc[num])\n",
    "    words.append(word)\n",
    "    \n",
    "pd.DataFrame({\n",
    "    \"words\" : words,\n",
    "    \"counts\" : numbers,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d71c93",
   "metadata": {},
   "source": [
    "### 2) Running the first models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a880f",
   "metadata": {},
   "source": [
    "#### Model number 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07155c94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 3,718,004 \n",
      "The amount of time it took to vectorize was: 17.154719999999998\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3751  386]\n",
      " [ 530 3583]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      4137\n",
      "           1       0.90      0.87      0.89      4113\n",
      "\n",
      "    accuracy                           0.89      8250\n",
      "   macro avg       0.89      0.89      0.89      8250\n",
      "weighted avg       0.89      0.89      0.89      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.9027462836986646\n",
      "Accuracy Score: 0.888969696969697\n",
      "Recall Score: 0.8711402868952103\n",
      "f1 Score 0.886661717396684\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "for cv ,clf, clf_name in [(vec , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(vec, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a1d26",
   "metadata": {},
   "source": [
    "This is the base Multinominal only using basic stop words in english. It is using a basic ngram_range of 1,1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc6e38d",
   "metadata": {},
   "source": [
    "#### Model number 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f4e570d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 2,991,762 \n",
      "The amount of time it took to vectorize was: 13.267948000000004\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3663  474]\n",
      " [ 545 3568]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88      4137\n",
      "           1       0.88      0.87      0.88      4113\n",
      "\n",
      "    accuracy                           0.88      8250\n",
      "   macro avg       0.88      0.88      0.88      8250\n",
      "weighted avg       0.88      0.88      0.88      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.8827313211281543\n",
      "Accuracy Score: 0.8764848484848485\n",
      "Recall Score: 0.8674933138828106\n",
      "f1 Score 0.8750459840588596\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "\n",
    "for vec ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    \n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(vec, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eba6da",
   "metadata": {},
   "source": [
    "The second model performs better, specially in recall. However it takes too much time to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1188f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model number 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d499e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 2,441,663 \n",
      "The amount of time it took to vectorize was: 10.759179000000017\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3653  484]\n",
      " [ 550 3563]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88      4137\n",
      "           1       0.88      0.87      0.87      4113\n",
      "\n",
      "    accuracy                           0.87      8250\n",
      "   macro avg       0.87      0.87      0.87      8250\n",
      "weighted avg       0.87      0.87      0.87      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.8804052384482333\n",
      "Accuracy Score: 0.8746666666666667\n",
      "Recall Score: 0.8662776562120107\n",
      "f1 Score 0.8732843137254902\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1, 3))\n",
    "X_train_sample = X_train.sample(frac=.8, random_state=50)\n",
    "y_train_sample = y_train.sample(frac=.8, random_state=50)\n",
    "\n",
    "for vec ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    \n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(vec, X_train_sample, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train_sample)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b6fa3",
   "metadata": {},
   "source": [
    "Feeding the model with less amount of data; seems to make the model run worse, which was expected. I need to check with an a/b testing though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc5a1c",
   "metadata": {},
   "source": [
    "#### Model number 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "635a74ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 5,342,619 \n",
      "The amount of time it took to vectorize was: 21.06203400000001\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3643  494]\n",
      " [ 527 3586]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88      4137\n",
      "           1       0.88      0.87      0.88      4113\n",
      "\n",
      "    accuracy                           0.88      8250\n",
      "   macro avg       0.88      0.88      0.88      8250\n",
      "weighted avg       0.88      0.88      0.88      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.878921568627451\n",
      "Accuracy Score: 0.8762424242424243\n",
      "Recall Score: 0.8718696814976903\n",
      "f1 Score 0.8753814231661174\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,5))\n",
    "\n",
    "for vec ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    \n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(vec, X_train_sample, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train_sample)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ecd56",
   "metadata": {},
   "source": [
    "#### Model number 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e2ef4d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 3,718,004 \n",
      "The amount of time it took to vectorize was: 16.878854000000004\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3751  386]\n",
      " [ 530 3583]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      4137\n",
      "           1       0.90      0.87      0.89      4113\n",
      "\n",
      "    accuracy                           0.89      8250\n",
      "   macro avg       0.89      0.89      0.89      8250\n",
      "weighted avg       0.89      0.89      0.89      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.9027462836986646\n",
      "Accuracy Score: 0.888969696969697\n",
      "Recall Score: 0.8711402868952103\n",
      "f1 Score 0.886661717396684\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "stop_words = 'the and of to is in br it that this as with for was film movie his on are have be one'.split(' ')\n",
    "cv = CountVectorizer(stop_words=stop_words, ngram_range=(1,3))\n",
    "\n",
    "for cv ,clf, clf_name in [(vec , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(vec, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e4097",
   "metadata": {},
   "source": [
    "This was a mechanical way to extract words but I have an idea of how to authomate the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe20d32",
   "metadata": {},
   "source": [
    "### 3) Tweeking Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2071e",
   "metadata": {},
   "source": [
    "#### Max df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40bc6d6",
   "metadata": {},
   "source": [
    "Max_df and min_df are use to eliminate words based on some threshold. The idea is that some tokens instead of helping\n",
    "the model are only noise.\n",
    "<br>There are two basic approaches to handle this types of words. We can eliminate very common words(using **max_df**) or we can eliminate very rare words using min_df.\n",
    "<br><br> The idea with max_df is that if a word appears at least in a certain percentage of the sample then it should be removed.\n",
    "<br><br> The idea with min_df is that if a word appears in less document that in the theshold it should be ignored. \n",
    "<br><br> Notice that max_df will have a lot of overlapping with stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4783ca4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the shape for 0.4\n",
      "(16750, 3713413)\n",
      "This is the shape for 0.5\n",
      "(16750, 3713428)\n",
      "This is the shape for 0.6\n",
      "(16750, 3713440)\n",
      "This is the shape for 0.7\n",
      "(16750, 3713446)\n",
      "This is the shape for 0.8\n",
      "(16750, 3713448)\n",
      "This is the shape for 0.9\n",
      "(16750, 3713452)\n",
      "This is the shape for 1.0\n",
      "(16750, 3713457)\n"
     ]
    }
   ],
   "source": [
    "for i in range(4,11,1):\n",
    "    count_vectorizer = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), max_df=i/10)\n",
    "    X_train_vectorized = count_vectorizer.fit_transform(X_train)\n",
    "    print(f'This is the shape for {i/10}')\n",
    "    print(X_train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9808131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the shape for 0.4\n",
      "(16750, 63655)\n",
      "This is the shape for 0.5\n",
      "(16750, 63670)\n",
      "This is the shape for 0.6\n",
      "(16750, 63680)\n",
      "This is the shape for 0.7\n",
      "(16750, 63685)\n",
      "This is the shape for 0.8\n",
      "(16750, 63687)\n",
      "This is the shape for 0.9\n",
      "(16750, 63691)\n",
      "This is the shape for 1.0\n",
      "(16750, 63696)\n"
     ]
    }
   ],
   "source": [
    "for i in range(4,11,1):\n",
    "    count_vectorizer = CountVectorizer(stop_words={'english'}, ngram_range=(1,1), max_df=i/10)\n",
    "    X_train_vectorized = count_vectorizer.fit_transform(X_train)\n",
    "    print(f'This is the shape for {i/10}')\n",
    "    print(X_train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c38a720",
   "metadata": {},
   "source": [
    "I don't see much of a difference in this max_df, even at really low percentages. But, never know if this can really affect the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "851aa8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>character</td>\n",
       "      <td>4746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>films</td>\n",
       "      <td>4627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>life</td>\n",
       "      <td>4399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>where</td>\n",
       "      <td>4350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>plot</td>\n",
       "      <td>4349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>little</td>\n",
       "      <td>4309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>show</td>\n",
       "      <td>4296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>love</td>\n",
       "      <td>4287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>over</td>\n",
       "      <td>4270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>best</td>\n",
       "      <td>4199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>did</td>\n",
       "      <td>4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>know</td>\n",
       "      <td>4167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>does</td>\n",
       "      <td>4065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>off</td>\n",
       "      <td>4059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ever</td>\n",
       "      <td>3982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>man</td>\n",
       "      <td>3980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>better</td>\n",
       "      <td>3936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>here</td>\n",
       "      <td>3884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>end</td>\n",
       "      <td>3807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>your</td>\n",
       "      <td>3794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  counts\n",
       "0   character    4746\n",
       "1       films    4627\n",
       "2        life    4399\n",
       "3       where    4350\n",
       "4        plot    4349\n",
       "5      little    4309\n",
       "6        show    4296\n",
       "7        love    4287\n",
       "8        over    4270\n",
       "9        best    4199\n",
       "10        did    4190\n",
       "11       know    4167\n",
       "12       does    4065\n",
       "13        off    4059\n",
       "14       ever    3982\n",
       "15        man    3980\n",
       "16     better    3936\n",
       "17       here    3884\n",
       "18        end    3807\n",
       "19       your    3794"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Most common words in general\n",
    "\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,1), max_df=.2)\n",
    "list_of_words = cv.fit_transform(X_train).toarray()\n",
    "sum_of_words = pd.Series(list_of_words.sum(axis=0))\n",
    "sum_of_words = sum_of_words.sort_values(ascending=False)[:20]\n",
    "\n",
    "words = []\n",
    "numbers = []\n",
    "\n",
    "for num in sum_of_words.index:\n",
    "    word = (list(cv.vocabulary_.keys())[list(cv.vocabulary_.values()).index(num)])\n",
    "    numbers.append(sum_of_words.loc[num])\n",
    "    words.append(word)\n",
    "    \n",
    "pd.DataFrame({\n",
    "    \"words\" : words,\n",
    "    \"counts\" : numbers,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8c2100",
   "metadata": {},
   "source": [
    "#### Model number 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca7d72fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 3,718,004 \n",
      "The amount of time it took to vectorize was: 18.143799\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3751  386]\n",
      " [ 530 3583]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      4137\n",
      "           1       0.90      0.87      0.89      4113\n",
      "\n",
      "    accuracy                           0.89      8250\n",
      "   macro avg       0.89      0.89      0.89      8250\n",
      "weighted avg       0.89      0.89      0.89      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.9027462836986646\n",
      "Accuracy Score: 0.888969696969697\n",
      "Recall Score: 0.8711402868952103\n",
      "f1 Score 0.886661717396684\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), max_df=.2 )\n",
    "\n",
    "for cv ,clf, clf_name in [(vec , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(vec, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "803f4ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 3,718,004 \n",
      "The amount of time it took to vectorize was: 17.942894000000024\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3751  386]\n",
      " [ 530 3583]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      4137\n",
      "           1       0.90      0.87      0.89      4113\n",
      "\n",
      "    accuracy                           0.89      8250\n",
      "   macro avg       0.89      0.89      0.89      8250\n",
      "weighted avg       0.89      0.89      0.89      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.9027462836986646\n",
      "Accuracy Score: 0.888969696969697\n",
      "Recall Score: 0.8711402868952103\n",
      "f1 Score 0.886661717396684\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), max_df=.3)\n",
    "\n",
    "for cv ,clf, clf_name in [(vec , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(vec, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7b73dd",
   "metadata": {},
   "source": [
    "This models perform quiet similar to the models that where run with the decided stop_words (which was expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e3cd0d",
   "metadata": {},
   "source": [
    "#### Model number 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ddfe39c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 3,718,004 \n",
      "The amount of time it took to vectorize was: 18.375955000000033\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3751  386]\n",
      " [ 530 3583]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      4137\n",
      "           1       0.90      0.87      0.89      4113\n",
      "\n",
      "    accuracy                           0.89      8250\n",
      "   macro avg       0.89      0.89      0.89      8250\n",
      "weighted avg       0.89      0.89      0.89      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.9027462836986646\n",
      "Accuracy Score: 0.888969696969697\n",
      "Recall Score: 0.8711402868952103\n",
      "f1 Score 0.886661717396684\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "stop_words = 'the and of to is in br it that this as with for was film movie his on are have be one'.split(' ')\n",
    "cv = CountVectorizer(stop_words=stop_words, ngram_range=(1,3), max_df=.3)\n",
    "\n",
    "\n",
    "for cv ,clf, clf_name in [(vec , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(vec, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34387d",
   "metadata": {},
   "source": [
    "There was a lot of overlap, and this little amount of words really make a difference in the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f83cc",
   "metadata": {},
   "source": [
    "#### Models using min_df alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75874e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the shape for 2\n",
      "(16750, 38025)\n",
      "This is the shape for 5\n",
      "(16750, 22483)\n",
      "This is the shape for 10\n",
      "(16750, 14804)\n",
      "This is the shape for 15\n",
      "(16750, 11432)\n",
      "This is the shape for 20\n",
      "(16750, 9375)\n",
      "This is the shape for 25\n",
      "(16750, 7969)\n",
      "This is the shape for 30\n",
      "(16750, 7034)\n",
      "This is the shape for 50\n",
      "(16750, 4803)\n"
     ]
    }
   ],
   "source": [
    "for i in [2,5,10,15,20,25,30,50]:\n",
    "    count_vectorizer = CountVectorizer(stop_words={'english'}, ngram_range=(1,1), min_df=i)\n",
    "    X_train_vectorized = count_vectorizer.fit_transform(X_train)\n",
    "    print(f'This is the shape for {i}')\n",
    "    print(X_train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c819647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the shape for 2\n",
      "(16750, 622962)\n",
      "This is the shape for 5\n",
      "(16750, 175106)\n",
      "This is the shape for 10\n",
      "(16750, 80501)\n",
      "This is the shape for 15\n",
      "(16750, 52307)\n",
      "This is the shape for 20\n",
      "(16750, 38496)\n",
      "This is the shape for 25\n",
      "(16750, 30253)\n",
      "This is the shape for 30\n",
      "(16750, 24837)\n",
      "This is the shape for 50\n",
      "(16750, 14359)\n"
     ]
    }
   ],
   "source": [
    "for i in [2,5,10,15,20,25,30,50]:\n",
    "    count_vectorizer = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), min_df=i)\n",
    "    X_train_vectorized = count_vectorizer.fit_transform(X_train)\n",
    "    print(f'This is the shape for {i}')\n",
    "    print(X_train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d8c3cff7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the shape for 2\n",
      "(16750, 408279)\n",
      "This is the shape for 5\n",
      "(16750, 99325)\n",
      "This is the shape for 10\n",
      "(16750, 43980)\n",
      "This is the shape for 15\n",
      "(16750, 28243)\n",
      "This is the shape for 20\n",
      "(16750, 20688)\n",
      "This is the shape for 25\n",
      "(16750, 16255)\n",
      "This is the shape for 30\n",
      "(16750, 13391)\n",
      "This is the shape for 50\n",
      "(16750, 7828)\n"
     ]
    }
   ],
   "source": [
    "for i in [2,5,10,15,20,25,30,50]:\n",
    "    count_vectorizer = CountVectorizer(stop_words=stop_words, ngram_range=(1,3), min_df=i)\n",
    "    X_train_vectorized = count_vectorizer.fit_transform(X_train)\n",
    "    print(f'This is the shape for {i}')\n",
    "    print(X_train_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6e0b08",
   "metadata": {},
   "source": [
    "#### Model number 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28c5b373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 99,325 \n",
      "The amount of time it took to vectorize was: 11.592953999999963\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3639  498]\n",
      " [ 505 3608]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      4137\n",
      "           1       0.88      0.88      0.88      4113\n",
      "\n",
      "    accuracy                           0.88      8250\n",
      "   macro avg       0.88      0.88      0.88      8250\n",
      "weighted avg       0.88      0.88      0.88      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.8787140769605456\n",
      "Accuracy Score: 0.8784242424242424\n",
      "Recall Score: 0.8772185752492099\n",
      "f1 Score 0.8779656892566006\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words=stop_words, ngram_range=(1,3), min_df=5)\n",
    "\n",
    "\n",
    "for cv ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(cv, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5048d13",
   "metadata": {},
   "source": [
    "#### Model number 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19a7983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 43,980 \n",
      "The amount of time it took to vectorize was: 10.890859999999975\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3592  545]\n",
      " [ 510 3603]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87      4137\n",
      "           1       0.87      0.88      0.87      4113\n",
      "\n",
      "    accuracy                           0.87      8250\n",
      "   macro avg       0.87      0.87      0.87      8250\n",
      "weighted avg       0.87      0.87      0.87      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.8686113789778206\n",
      "Accuracy Score: 0.8721212121212121\n",
      "Recall Score: 0.8760029175784099\n",
      "f1 Score 0.8722914901343661\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words=stop_words, ngram_range=(1,3), min_df=10)\n",
    "\n",
    "\n",
    "for cv ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(cv, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748d255",
   "metadata": {},
   "source": [
    "Lets try some mixture of both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca693001",
   "metadata": {},
   "source": [
    "#### Model number 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3e6c60c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 80,433 \n",
      "The amount of time it took to vectorize was: 13.848969000000011\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3630  507]\n",
      " [ 445 3668]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88      4137\n",
      "           1       0.88      0.89      0.89      4113\n",
      "\n",
      "    accuracy                           0.88      8250\n",
      "   macro avg       0.88      0.88      0.88      8250\n",
      "weighted avg       0.88      0.88      0.88      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.878562874251497\n",
      "Accuracy Score: 0.8846060606060606\n",
      "Recall Score: 0.8918064672988086\n",
      "f1 Score 0.8851351351351352\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), min_df=10, max_df=0.3)\n",
    "\n",
    "\n",
    "for cv ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(cv, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d2f446",
   "metadata": {},
   "source": [
    "#### Model number 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4b09b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 80,457 \n",
      "The amount of time it took to vectorize was: 13.599285000000009\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3628  509]\n",
      " [ 449 3664]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88      4137\n",
      "           1       0.88      0.89      0.88      4113\n",
      "\n",
      "    accuracy                           0.88      8250\n",
      "   macro avg       0.88      0.88      0.88      8250\n",
      "weighted avg       0.88      0.88      0.88      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.8780254013898874\n",
      "Accuracy Score: 0.8838787878787879\n",
      "Recall Score: 0.8908339411621687\n",
      "f1 Score 0.8843832971276853\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), min_df=10, max_df=0.4)\n",
    "\n",
    "\n",
    "for cv ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(cv, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb10688",
   "metadata": {},
   "source": [
    "The combination of both doesn't look better (need more testing to check if it is the same) but it may be faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c1dde",
   "metadata": {},
   "source": [
    "#### Model number 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5db7ebc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 175,062 \n",
      "The amount of time it took to vectorize was: 13.904557000000068\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3661  476]\n",
      " [ 474 3639]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89      4137\n",
      "           1       0.88      0.88      0.88      4113\n",
      "\n",
      "    accuracy                           0.88      8250\n",
      "   macro avg       0.88      0.88      0.88      8250\n",
      "weighted avg       0.88      0.88      0.88      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.884325637910085\n",
      "Accuracy Score: 0.8848484848484849\n",
      "Recall Score: 0.8847556528081693\n",
      "f1 Score 0.8845405930967428\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), min_df=5, max_df=0.4)\n",
    "\n",
    "\n",
    "for cv ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(cv, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1301727",
   "metadata": {},
   "source": [
    "#### Model number 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7af35965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of features in the Vectorized X train is: 622,918 \n",
      "The amount of time it took to vectorize was: 15.014494000000013\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Confusion Matrix\n",
      "[[3704  433]\n",
      " [ 467 3646]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89      4137\n",
      "           1       0.89      0.89      0.89      4113\n",
      "\n",
      "    accuracy                           0.89      8250\n",
      "   macro avg       0.89      0.89      0.89      8250\n",
      "weighted avg       0.89      0.89      0.89      8250\n",
      "\n",
      "\n",
      "\n",
      "Other Metrics:\n",
      "Pression Score: 0.8938465310125031\n",
      "Accuracy Score: 0.8909090909090909\n",
      "Recall Score: 0.886457573547289\n",
      "f1 Score 0.89013671875\n",
      "_________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), min_df=2, max_df=0.4)\n",
    "\n",
    "\n",
    "for cv ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "    # Vectorization\n",
    "    start = time.process_time()\n",
    "    X_train_vectorized, X_test_vectorized = vectorize_X(cv, X_train, X_test)\n",
    "    print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "    print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "    \n",
    "    # Running the model\n",
    "    print(clf_name)\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "    check_model(clf, X_test_vectorized, y_test)\n",
    "    print(\"_________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530642cd",
   "metadata": {},
   "source": [
    "I would like to understand exactly why when you start to increase the min_df you get a worse score if also you are working with a max_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959686cc",
   "metadata": {},
   "source": [
    "### 3.5) Tailored based stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8cc68b",
   "metadata": {},
   "source": [
    "### 4) Contrasting Models and A/B testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0b606db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie is another Christian propaganda fil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A woman who hates cats (Alice Krige) and her s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beast Wars is a show that is over-hyped, overp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An excellent example of \"cowboy noir\", as it's...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ok, basically this is a popcorn sci-fi movie, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>Jimmy Cagney races by your eyes constantly in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>Very much a film from the times -- extremely l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>The Little Mermaid is one of my absolute favor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>With a simplistic story and an engaging heroin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>The 63 year reign of Queen Victoria is perhaps...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                contents  labels\n",
       "0      This movie is another Christian propaganda fil...       0\n",
       "1      A woman who hates cats (Alice Krige) and her s...       1\n",
       "2      Beast Wars is a show that is over-hyped, overp...       0\n",
       "3      An excellent example of \"cowboy noir\", as it's...       1\n",
       "4      Ok, basically this is a popcorn sci-fi movie, ...       1\n",
       "...                                                  ...     ...\n",
       "24995  Jimmy Cagney races by your eyes constantly in ...       1\n",
       "24996  Very much a film from the times -- extremely l...       0\n",
       "24997  The Little Mermaid is one of my absolute favor...       0\n",
       "24998  With a simplistic story and an engaging heroin...       1\n",
       "24999  The 63 year reign of Queen Victoria is perhaps...       1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83f69a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "for i1,i2 in kf.split(data):\n",
    "    lst1 = list(i1)\n",
    "    lst2 = list(i2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa6687d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "cv = CountVectorizer(stop_words={'english'}, ngram_range=(1,3), min_df=2, max_df=0.4)\n",
    "X = 1\n",
    "y = 1\n",
    "test_split = 1\n",
    "\n",
    "def model_run(cv, clf, model_name, X, y, test_split):\n",
    "    for cv ,clf, clf_name in [(cv , mnb, 'Multinomial Naive Bayes')]:\n",
    "        # Vectorization\n",
    "        start = time.process_time()\n",
    "        X_train_vectorized, X_test_vectorized = vectorize_X(cv, X_train, X_test)\n",
    "        print('The amount of features in the Vectorized X train is: {:,} '.format(X_train_vectorized.shape[1]))\n",
    "        print(f'The amount of time it took to vectorize was: {time.process_time() - start}\\n')\n",
    "\n",
    "        # Running the model\n",
    "        print(clf_name)\n",
    "        clf.fit(X_train_vectorized, y_train)\n",
    "        check_model(clf, X_test_vectorized, y_test)\n",
    "        print(\"_________________________________________________\")\n",
    "        \n",
    "def get_test_split():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "014b6dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12500</th>\n",
       "      <td>argh! this film hurts my head. and not in a go...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12501</th>\n",
       "      <td>Istanbul is another one of those expatriate fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12502</th>\n",
       "      <td>As a history nut who is particularly intereste...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12503</th>\n",
       "      <td>This is a very enjoyable film with excellent a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12504</th>\n",
       "      <td>Me being of Irish origins, loved this movie, N...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>Jimmy Cagney races by your eyes constantly in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>Very much a film from the times -- extremely l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>The Little Mermaid is one of my absolute favor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>With a simplistic story and an engaging heroin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>The 63 year reign of Queen Victoria is perhaps...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12500 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                contents  labels\n",
       "12500  argh! this film hurts my head. and not in a go...       0\n",
       "12501  Istanbul is another one of those expatriate fi...       0\n",
       "12502  As a history nut who is particularly intereste...       0\n",
       "12503  This is a very enjoyable film with excellent a...       1\n",
       "12504  Me being of Irish origins, loved this movie, N...       1\n",
       "...                                                  ...     ...\n",
       "24995  Jimmy Cagney races by your eyes constantly in ...       1\n",
       "24996  Very much a film from the times -- extremely l...       0\n",
       "24997  The Little Mermaid is one of my absolute favor...       0\n",
       "24998  With a simplistic story and an engaging heroin...       1\n",
       "24999  The 63 year reign of Queen Victoria is perhaps...       1\n",
       "\n",
       "[12500 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[~data.index.isin(i1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
